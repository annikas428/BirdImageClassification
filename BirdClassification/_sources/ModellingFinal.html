
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Notebook 2 - Modellierung eines Classifiers durch Supervised Learning &#8212; Bird Image Classification</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/theme.css" />
    <link rel="stylesheet" type="text/css" href="../_static/vendor/fontawesome/5.13.0/css/all.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/copybutton_funcs.js"></script>
    <script src="../_static/design-tabs.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery-3.5.1.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/searchtools.js"></script>
    <script src="../_static/sphinx-thebe.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/underscore-1.13.1.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/scripts/pydata-sphinx-theme.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Bird Image Classification</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Klassifizierung von Vogelbildern mit Neuronalen Netzen
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Datengewinnung.html">
   Notebook 1 - Datengewinnung
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ModellingFinal.html">
   Notebook 2 - Modellierung eines Classifiers durch Supervised Learning
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/annikas428/BirdImageClassification/master?urlpath=tree/docs/_sources/ModellingFinal.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/annikas428/BirdImageClassification/settings/pages"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/annikas428/BirdImageClassification/settings/pages/issues/new?title=Issue%20on%20page%20%2F_sources/ModellingFinal.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/_sources/ModellingFinal.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Notebook 2 - Modellierung eines Classifiers durch Supervised Learning
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#einleitung">
   1. Einleitung
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   2. Setup
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#import-of-datasets">
   3. Import of Datasets
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#erzeugen-von-dataset-1">
     3.1 Erzeugen von Dataset 1
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#erzeugen-von-dataset-2">
     3.2 Erzeugen von Dataset 2
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#erzeugen-von-dataset-3">
     3.3 Erzeugen von Dataset 3
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bilderexploration">
   4. Bilderexploration
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#image-augmentation">
   5. Image Augmentation
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#building-training-a-cnn-model-from-scratch">
   6. Building &amp; Training a CNN Model from Scratch
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-1-simples-cnn">
     6.1 Model 1 - simples CNN
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-model-1-introduction">
       a) Model 1 - Introduction
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#b-model-1-construction">
       b) Model 1 - Construction
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#c-model-1-training">
       c) Model 1 - Training
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#d-model-1-evaluation">
       d) Model 1 Evaluation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#e-model-1-fazit">
       e) Model 1 - Fazit
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-2-komplexeres-cnn">
     6.2 Model 2 - komplexeres CNN
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-model-2-introduction">
       a) Model 2 - Introduction
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#b-model-2-construction">
       b) Model 2 - Construction
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#c-model-2-training">
       c) Model 2 - Training
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#d-model-2-evaluation">
       d) Model 2 Evaluation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#e-model-2-fazit">
       e) Model 2 - Fazit
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-3">
     6.3 Model 3
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#c-model-3-introduction">
       c) Model 3 - Introduction
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-model-3-construction">
       a) Model 3 - Construction
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#b-model-3-training">
       b) Model 3 - Training
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-model-3-evaluation">
       a) Model 3 - Evaluation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#e-model-3-fazit">
       e) Model 3 - Fazit
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-4-pretrained-vgg16-neural-network">
     6.4 Model 4 - Pretrained VGG16 Neural Network
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-vgg16-introduction">
       a) VGG16 - Introduction
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#b-vgg16-construction">
       b) VGG16 - Construction
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#c-vgg16-training">
       c) VGG16 - Training
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#d-vgg16-evaluation">
       d) VGG16 - Evaluation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#e-vgg16-fazit">
       e) VGG16 - Fazit
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-5-pretrained-xception-neural-network">
     6.5 Model 5 - Pretrained Xception Neural Network
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-xception-introduction">
       a) Xception - Introduction
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#b-xception-construction">
       b) Xception - Construction
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#c-xception-training">
       c) Xception - Training
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#d-xception-evaluation">
       d) Xception - Evaluation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#e-xception-fazit">
       e) Xception - Fazit
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#projekt-fazit">
   7. Projekt Fazit
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quellen">
   8. Quellen
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Notebook 2 - Modellierung eines Classifiers durch Supervised Learning</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Notebook 2 - Modellierung eines Classifiers durch Supervised Learning
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#einleitung">
   1. Einleitung
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   2. Setup
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#import-of-datasets">
   3. Import of Datasets
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#erzeugen-von-dataset-1">
     3.1 Erzeugen von Dataset 1
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#erzeugen-von-dataset-2">
     3.2 Erzeugen von Dataset 2
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#erzeugen-von-dataset-3">
     3.3 Erzeugen von Dataset 3
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bilderexploration">
   4. Bilderexploration
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#image-augmentation">
   5. Image Augmentation
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#building-training-a-cnn-model-from-scratch">
   6. Building &amp; Training a CNN Model from Scratch
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-1-simples-cnn">
     6.1 Model 1 - simples CNN
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-model-1-introduction">
       a) Model 1 - Introduction
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#b-model-1-construction">
       b) Model 1 - Construction
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#c-model-1-training">
       c) Model 1 - Training
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#d-model-1-evaluation">
       d) Model 1 Evaluation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#e-model-1-fazit">
       e) Model 1 - Fazit
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-2-komplexeres-cnn">
     6.2 Model 2 - komplexeres CNN
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-model-2-introduction">
       a) Model 2 - Introduction
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#b-model-2-construction">
       b) Model 2 - Construction
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#c-model-2-training">
       c) Model 2 - Training
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#d-model-2-evaluation">
       d) Model 2 Evaluation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#e-model-2-fazit">
       e) Model 2 - Fazit
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-3">
     6.3 Model 3
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#c-model-3-introduction">
       c) Model 3 - Introduction
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-model-3-construction">
       a) Model 3 - Construction
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#b-model-3-training">
       b) Model 3 - Training
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-model-3-evaluation">
       a) Model 3 - Evaluation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#e-model-3-fazit">
       e) Model 3 - Fazit
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-4-pretrained-vgg16-neural-network">
     6.4 Model 4 - Pretrained VGG16 Neural Network
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-vgg16-introduction">
       a) VGG16 - Introduction
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#b-vgg16-construction">
       b) VGG16 - Construction
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#c-vgg16-training">
       c) VGG16 - Training
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#d-vgg16-evaluation">
       d) VGG16 - Evaluation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#e-vgg16-fazit">
       e) VGG16 - Fazit
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-5-pretrained-xception-neural-network">
     6.5 Model 5 - Pretrained Xception Neural Network
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-xception-introduction">
       a) Xception - Introduction
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#b-xception-construction">
       b) Xception - Construction
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#c-xception-training">
       c) Xception - Training
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#d-xception-evaluation">
       d) Xception - Evaluation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#e-xception-fazit">
       e) Xception - Fazit
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#projekt-fazit">
   7. Projekt Fazit
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quellen">
   8. Quellen
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="notebook-2-modellierung-eines-classifiers-durch-supervised-learning">
<h1>Notebook 2 - Modellierung eines Classifiers durch Supervised Learning<a class="headerlink" href="#notebook-2-modellierung-eines-classifiers-durch-supervised-learning" title="Permalink to this headline">#</a></h1>
<p>von Annika Scheug und Oliver Schabe</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="einleitung">
<h1>1. Einleitung<a class="headerlink" href="#einleitung" title="Permalink to this headline">#</a></h1>
<p>Wie bereits in Notebook 1 erwähnt, soll in diesem Projekt ein Neuronales Netz modelliert werden, welches Bilder verschiedener Vogelarten klassifizieren kann (Multiclass Classification).<br />
Hierfür verwenden wir Convolutional Neural Networks (CNN), da sich diese nach verbreiteter Meinung am besten zur Klassifizierung von Bildern in mehrere Klassen eignen.<br />
Wie in Notebook 1 bereits beschrieben, sollen folgende Vogelarten klassifiziert werden, um Anspruch und Machbarkeit im Rahmen zu halten (mehr dazu in Notebook 1):</p>
<ul class="simple">
<li><p>Huhn</p></li>
<li><p>Kakadu</p></li>
<li><p>Adler</p></li>
<li><p>Flamingo</p></li>
<li><p>Strauß</p></li>
<li><p>Eule</p></li>
<li><p>Pinguin</p></li>
<li><p>Meise</p></li>
<li><p>Tucan</p></li>
</ul>
<p>Für unser vollständig selbst entwickeltes Modell streben wir eine Accuracy von mind. 80% an.<br />
Zum Vergleich sollen außerdem vortrainierte Netze herangezogen und für den hier beschriebenen Anwendungsfall optmiert werden. Hier wird mit einer höheren Trefferquote als bei dem vollständig selbst modellierten Netz gerechnet.</p>
<p><strong>Task:</strong> Die Aufgabe unseres Modells ist die Klassifikation, also Erkennung von neun definierten Vogelarten in Bildern.</p>
<p><strong>Experience:</strong> Das Modell soll aus mehreren Tausend Bildern lernen, auf denen jeweils ein Vogel der definierten Vogelarten abgebildet ist und welches entsprechend gelabelt ist.</p>
<p><strong>Performance:</strong> Das Modell soll an dem Kennwert Accuracy gemessen werden, da diese die intuitivste Metrik ist und uns sagt, wie viele der Testbilder richtig erkannt wurden. Hierbei ist jedoch wichtig, dass nicht nur die gesamte Accuracy des Modells, sondern auch Performance pro Klasse betrachtet wird. Dazu siehen wir Precision, Recall und F1-Score heran. Dadurch soll soll erkannt werden, falls einzelne Klassen, welche evtl. sogar noch unterrepräsentiert sind, außergewöhnlich schlecht erkannt werden.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="setup">
<h1>2. Setup<a class="headerlink" href="#setup" title="Permalink to this headline">#</a></h1>
<p>Zunächst werden alle für das Projekt wichtigen Funktionalitäten importiert.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">image_dataset_from_directory</span>

<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span><span class="p">,</span> <span class="n">Model</span><span class="p">,</span> <span class="n">load_model</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span><span class="n">Input</span><span class="p">,</span><span class="n">Dropout</span><span class="p">,</span><span class="n">Flatten</span><span class="p">,</span><span class="n">Conv2D</span><span class="p">,</span><span class="n">MaxPool2D</span><span class="p">,</span><span class="n">RandomFlip</span><span class="p">,</span><span class="n">RandomRotation</span><span class="p">,</span><span class="n">Rescaling</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">optimizers</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">confusion_matrix</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.applications</span> <span class="kn">import</span> <span class="n">vgg16</span><span class="p">,</span> <span class="n">xception</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.utils</span> <span class="kn">import</span> <span class="n">load_img</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.utils</span> <span class="kn">import</span> <span class="n">img_to_array</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">expand_dims</span>
<span class="kn">import</span> <span class="nn">operator</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">utilsJM</span> <span class="kn">import</span> <span class="n">plot_confusion_matrix</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span> 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Für Visualisierung CNN</span>
<span class="c1">#pip install visualkeras</span>
<span class="kn">import</span> <span class="nn">visualkeras</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">ImageFont</span>
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="import-of-datasets">
<h1>3. Import of Datasets<a class="headerlink" href="#import-of-datasets" title="Permalink to this headline">#</a></h1>
<p>Da verschiedene Modelle mit verschiedenen Datasets (unterschiedliche Bilder, Bildgröße und Batch Size) getestet werden sollen, wird zunächst eine Funktion erstellt, welche die Bilder vom angegebenen Pfad in der angegebenen Auflösung und Batch Size in ein Trainings Datenset (train_ds) und ein Validation Datenset (val_ds) lädt.<br />
Da die verwendete Keras util “image_dataset_from_directory” nur die Möglichkeit bietet, die eingelesenen Daten in zwei Datensets aufzuteilen, wird das Validation Datenset danach noch in Validation und Test Daten aufgeteilt.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Funktion zum einlesen der Bilder und erstellen der Datensets</span>
<span class="k">def</span> <span class="nf">create_datasets</span><span class="p">(</span><span class="n">image_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">directory</span><span class="p">):</span>

    <span class="n">train_ds</span> <span class="o">=</span> <span class="n">image_dataset_from_directory</span><span class="p">(</span>
        <span class="n">directory</span><span class="p">,</span>
        <span class="n">label_mode</span><span class="o">=</span><span class="s2">&quot;categorical&quot;</span><span class="p">,</span>
        <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
        <span class="n">subset</span><span class="o">=</span><span class="s2">&quot;training&quot;</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="mi">1337</span><span class="p">,</span>
        <span class="n">image_size</span><span class="o">=</span><span class="n">image_size</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">val_ds</span> <span class="o">=</span> <span class="n">image_dataset_from_directory</span><span class="p">(</span>
        <span class="n">directory</span><span class="p">,</span>
        <span class="n">label_mode</span><span class="o">=</span><span class="s2">&quot;categorical&quot;</span><span class="p">,</span>
        <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
        <span class="n">subset</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="mi">1337</span><span class="p">,</span>
        <span class="n">image_size</span><span class="o">=</span><span class="n">image_size</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">train_ds</span><span class="p">,</span> <span class="n">val_ds</span>
</pre></div>
</div>
</div>
</div>
<p>Nach den ersten Tests fiel bereits auf, dass die Ergebnisse, welche mit der in Notebook 1 entwickleten Methode abgezogenenen Bildern erzeugt wurden, nicht zufriedenstellend sind.
Um zu vergleichen, ob die schlechten Ergebnisse an den Bildern oder am entwickelten Modell liegen, werden im folgenden drei Datasets erzeugt. Mit diesen drei Datasets sollen die Modell trainiert und getestet werden:</p>
<p><strong>Dataset 1:</strong> Nur Bilder, welche in Notebook 1 erzeugt wurden</p>
<p><strong>Dataset 2:</strong> Bilder aus Notebook 1, mit Bilder Datensets aus dem Internet angereichert (<a class="reference external" href="https://images.cv/">https://images.cv/</a>)</p>
<p><strong>Dataset 3:</strong> Nur Bilder Datensets aus dem Internet (<a class="reference external" href="https://images.cv/">https://images.cv/</a>)</p>
<section id="erzeugen-von-dataset-1">
<h2>3.1 Erzeugen von Dataset 1<a class="headerlink" href="#erzeugen-von-dataset-1" title="Permalink to this headline">#</a></h2>
<p>Als erstes wird das Dataset erzeugt, welches nur aus den in Notebook 1 generierten Bildern besteht. Dazu wird zunächst die Funktion mit den gewählten Parametern aufgerufen.<br />
Für die Parameter image_size und batch_size wurden verschiedene Werte getestet (bspw. batch_size von 64 und 128 oder Auflösung von 32x32 und 128x128), die hier abgebildeten Werte stellen für die meisten Modelle die optimal gewählten Parameter dar. Falls für ein trainiertes Modell final andere Parameter gewählt wurden, so ist dies in der Beschreibung des Modells weiter unten vermerkt.<br />
Das hier beschriebene Vorgehen ist identisch für Dataset 2 und Dataset 3, lediglich der Pfad zu den entsprechenden Bildern unterscheidet sich.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Aufruf der Funktion zum erstellen der Datasets Test und Validation</span>
<span class="n">image_size1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span>
<span class="n">batch_size1</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">train_ds1</span><span class="p">,</span> <span class="n">val_ds1</span> <span class="o">=</span> <span class="n">create_datasets</span><span class="p">(</span><span class="n">image_size1</span><span class="p">,</span> <span class="n">batch_size1</span><span class="p">,</span><span class="s2">&quot;..\ProjectBirdClassification\Bilder1&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Found 6457 files belonging to 9 classes.
Using 4520 files for training.
Found 6457 files belonging to 9 classes.
Using 1937 files for validation.
</pre></div>
</div>
</div>
</div>
<p>Mit Hilfe von “element_spec” können wir die Struktur des gerade erzeugten datasets überprüfen:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_ds1</span><span class="o">.</span><span class="n">element_spec</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(TensorSpec(shape=(None, 64, 64, 3), dtype=tf.float32, name=None),
 TensorSpec(shape=(None, 9), dtype=tf.float32, name=None))
</pre></div>
</div>
</div>
</div>
<p>Als Nächstes wird aus der ersten Hälfte des Validation Datasets (val_ds) ein Test Dataset (test_ds) erstellt:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_ds1</span> <span class="o">=</span> <span class="n">val_ds1</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">31</span><span class="p">)</span>
<span class="n">val_ds1</span> <span class="o">=</span> <span class="n">val_ds1</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="mi">31</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Batches for testing --&gt;&#39;</span><span class="p">,</span> <span class="n">test_ds1</span><span class="o">.</span><span class="n">cardinality</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Batches for validating --&gt;&#39;</span><span class="p">,</span> <span class="n">val_ds1</span><span class="o">.</span><span class="n">cardinality</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Batches for testing --&gt; tf.Tensor(31, shape=(), dtype=int64)
Batches for validating --&gt; tf.Tensor(30, shape=(), dtype=int64)
</pre></div>
</div>
</div>
</div>
<p>Nun werden die Namen der Klassen ausgegeben.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Die Klassen sind in allen drei datasets gleich und müssen daher nur einmal erzeugt werden</span>
<span class="n">class_names</span> <span class="o">=</span> <span class="n">train_ds1</span><span class="o">.</span><span class="n">class_names</span>
<span class="n">class_names</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;chicken&#39;,
 &#39;cockatoo&#39;,
 &#39;eagle&#39;,
 &#39;flamingo&#39;,
 &#39;ostrich&#39;,
 &#39;owl&#39;,
 &#39;penguin&#39;,
 &#39;tit&#39;,
 &#39;tucan&#39;]
</pre></div>
</div>
</div>
</div>
<p>Nachfolgend iterieren wir über das Datenset, um aus den one hot encoded labeln einen Vektor mit einem integer Wert je Klasse zu gewinnen:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_train_class1</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_ds1</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
        <span class="n">y_train_class1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Anschließend wird in einer weiteren Schleife gezählt, wie oft jede Klasse im Datenset enthalten ist:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">class_count1</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">)):</span>
    <span class="n">class_count1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_train_class1</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>
 
<span class="nb">print</span><span class="p">(</span><span class="n">class_count1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[470, 502, 526, 476, 512, 518, 507, 510, 499]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plotten der gezälten Bilder</span>
<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">)),</span> <span class="n">y</span><span class="o">=</span><span class="n">class_count1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Number of images per class&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Number of images per class&#39;)
</pre></div>
</div>
<img alt="../_images/ModellingFinal_27_1.png" src="../_images/ModellingFinal_27_1.png" />
</div>
</div>
<p>In den Trainingsdaten von Dataset 1 liegen je Klasse zwischen 470 und 526 Bilder vor. Die Anzahl der Bilder pro Klasse sind ungefähr gleich verteilt.</p>
</section>
<section id="erzeugen-von-dataset-2">
<h2>3.2 Erzeugen von Dataset 2<a class="headerlink" href="#erzeugen-von-dataset-2" title="Permalink to this headline">#</a></h2>
<p>Hier wird Dataset 2 erzeugt, welches aus den selbst erzeugten Bidler aus Notebook 1 besteht und mit Bilder Datasets aus dem Internet angereichert wurde (<a class="reference external" href="https://images.cv/">https://images.cv/</a>).<br />
Dataset 2 wird auf die gleiche Weise erzeugt wie Dataset 1, siehe Abschnitt 3.1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Aufruf der Funktion zum erstellen der Datasets Test und Validation</span>
<span class="n">image_size2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span>
<span class="n">batch_size2</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">train_ds2</span><span class="p">,</span> <span class="n">val_ds2</span> <span class="o">=</span> <span class="n">create_datasets</span><span class="p">(</span><span class="n">image_size2</span><span class="p">,</span> <span class="n">batch_size2</span><span class="p">,</span> <span class="s2">&quot;..\ProjectBirdClassification\Bilder2&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Found 14500 files belonging to 9 classes.
Using 10150 files for training.
Found 14500 files belonging to 9 classes.
Using 4350 files for validation.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_ds2</span> <span class="o">=</span> <span class="n">val_ds2</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">68</span><span class="p">)</span>
<span class="n">val_ds2</span> <span class="o">=</span> <span class="n">val_ds2</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="mi">68</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Batches for testing --&gt;&#39;</span><span class="p">,</span> <span class="n">test_ds2</span><span class="o">.</span><span class="n">cardinality</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Batches for validating --&gt;&#39;</span><span class="p">,</span> <span class="n">val_ds2</span><span class="o">.</span><span class="n">cardinality</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Batches for testing --&gt; tf.Tensor(68, shape=(), dtype=int64)
Batches for validating --&gt; tf.Tensor(68, shape=(), dtype=int64)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_train_class2</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_ds2</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
        <span class="n">y_train_class2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">class_count2</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">)):</span>
    <span class="n">class_count2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_train_class2</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>
 
<span class="nb">print</span><span class="p">(</span><span class="n">class_count2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[737, 784, 1080, 1295, 1293, 1129, 933, 1490, 1409]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plotten der gezälten Bilder</span>
<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">)),</span> <span class="n">y</span><span class="o">=</span><span class="n">class_count2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Number of images per class&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Number of images per class&#39;)
</pre></div>
</div>
<img alt="../_images/ModellingFinal_35_1.png" src="../_images/ModellingFinal_35_1.png" />
</div>
</div>
<p>In den Trainingsdaten von Dataset 2 liegen mehr Bilder vor als in Dataset 1, da hier zusätzliche die Bilder von <a class="reference external" href="https://images.cv/">https://images.cv/</a> verwendet wurden. Auch die Anzahl der Bilder je Klasse weicht stärker voneinander ab.<br />
Klasse 0 (Chicken/Huhn) zum Beispiel ist nur mit 737 Bildern vertreten, während für Klasse 7 (Tit/Meise) 1409 Bilder vorliegen.<br />
Dabei besteht die Gefahr, dass Modelle auf Basis dieser Trainingsdaten biased gegenüber der stärker vertretenen Klassen sind oder schlechte Erkennung von schwach vertretenen Klassen in der gesamten Accuracy des Modells untergehen.<br />
Da später jedoch auch Metriken je Klasse getrackt werden sollen, werden die Bilder alle als Traingsmaterial für das Modell beibehalten.</p>
</section>
<section id="erzeugen-von-dataset-3">
<h2>3.3 Erzeugen von Dataset 3<a class="headerlink" href="#erzeugen-von-dataset-3" title="Permalink to this headline">#</a></h2>
<p>Hier wird Dataset 3 erzeugt, welches nur aus den Bilder Datasets aus dem Internet besteht (<a class="reference external" href="https://images.cv/">https://images.cv/</a>).<br />
Dataset 3 wird auf die gleiche Weise erzeugt wie Dataset 1, siehe Abschnitt 3.1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Aufruf der Funktion zum erstellen der Datasets Test und Validation</span>
<span class="n">image_size3</span> <span class="o">=</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span>
<span class="n">batch_size3</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">train_ds3</span><span class="p">,</span> <span class="n">val_ds3</span> <span class="o">=</span> <span class="n">create_datasets</span><span class="p">(</span><span class="n">image_size3</span><span class="p">,</span> <span class="n">batch_size3</span><span class="p">,</span> <span class="s2">&quot;..\ProjectBirdClassification\Bilder3&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Found 9682 files belonging to 9 classes.
Using 6778 files for training.
Found 9682 files belonging to 9 classes.
Using 2904 files for validation.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_ds3</span> <span class="o">=</span> <span class="n">val_ds3</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">46</span><span class="p">)</span>
<span class="n">val_ds3</span> <span class="o">=</span> <span class="n">val_ds3</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="mi">46</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Batches for testing --&gt;&#39;</span><span class="p">,</span> <span class="n">test_ds3</span><span class="o">.</span><span class="n">cardinality</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Batches for validating --&gt;&#39;</span><span class="p">,</span> <span class="n">val_ds3</span><span class="o">.</span><span class="n">cardinality</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Batches for testing --&gt; tf.Tensor(46, shape=(), dtype=int64)
Batches for validating --&gt; tf.Tensor(45, shape=(), dtype=int64)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_train_class3</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_ds3</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
        <span class="n">y_train_class3</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">class_count3</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">)):</span>
    <span class="n">class_count3</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_train_class3</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>
 
<span class="nb">print</span><span class="p">(</span><span class="n">class_count3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[804, 740, 535, 748, 696, 603, 810, 926, 916]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plotten der gezälten Bilder</span>
<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">)),</span> <span class="n">y</span><span class="o">=</span><span class="n">class_count3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Number of images per class&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Number of images per class&#39;)
</pre></div>
</div>
<img alt="../_images/ModellingFinal_43_1.png" src="../_images/ModellingFinal_43_1.png" />
</div>
</div>
<p>Ähnlich wie bei Dataset 2 sind hier unterschiedlich viele Bilder je Label enthalten. Aus den selben Gründen wie bei Dataset 2 sollen die Bilder so behalten werden.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="bilderexploration">
<h1>4. Bilderexploration<a class="headerlink" href="#bilderexploration" title="Permalink to this headline">#</a></h1>
<p>In diesem Kapitel geht es darum, ein Gefühl für die Daten zu bekommen, indem Bilder der verschiedenen Klassen visualisiert werden.</p>
<p>Hierfür wollen wir mit dem folgenden Code zunächst ein Bild jeder Klasse mit dem dazugehörigen Label anzeigen. Dafür wird in einer Schleife durch die ersten zwei Batches des Datasets iteriert und ein Bild geplotted, sofern nicht bereits in einer vorherigen Iteration bereits ein Bild dieser Klasse geplotted wurde. Dies realisieren wir mit der Liste <em>plotted</em>. Wird ein Bild einer Klasse geplotted, wird am enstprechenden Index der Klassennummer der Wert von <em>plotted</em> auf ‘plotted’ gesetzt. Bevor ein Bild geplotted wird, wird in einer if-Bedingung geprüft, ob diese Klasse bereits geplotted wurde. Falls nicht, wird das Bild geplotted und anschließend <em>plotted</em> upgedated, falls ja wird mit dem nächsten Element weitergemacht.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">pos</span><span class="o">=</span><span class="mi">0</span>
<span class="n">plotted</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_ds1</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size1</span><span class="p">):</span>
        <span class="k">if</span><span class="p">(</span><span class="n">plotted</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)]</span> <span class="o">!=</span> <span class="s1">&#39;plotted&#39;</span><span class="p">):</span>
            <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">pos</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint8&quot;</span><span class="p">))</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">class_names</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)])</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
            <span class="n">plotted</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)]</span> <span class="o">=</span> <span class="s1">&#39;plotted&#39;</span>
            <span class="n">pos</span><span class="o">=</span><span class="n">pos</span><span class="o">+</span><span class="mi">1</span>        
        <span class="k">else</span><span class="p">:</span>
            <span class="k">continue</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_48_0.png" src="../_images/ModellingFinal_48_0.png" />
</div>
</div>
<p>Die oben beschriebene Visualisierung eines Bilds jeder Klasse wird für das dataset 2 wiederholt.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">pos</span><span class="o">=</span><span class="mi">0</span>
<span class="n">plotted</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_ds2</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size3</span><span class="p">):</span>
        <span class="k">if</span><span class="p">(</span><span class="n">plotted</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)]</span> <span class="o">!=</span> <span class="s1">&#39;plotted&#39;</span><span class="p">):</span>
            <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">pos</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint8&quot;</span><span class="p">))</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">class_names</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)])</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
            <span class="n">plotted</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)]</span> <span class="o">=</span> <span class="s1">&#39;plotted&#39;</span>
            <span class="n">pos</span><span class="o">=</span><span class="n">pos</span><span class="o">+</span><span class="mi">1</span>        
        <span class="k">else</span><span class="p">:</span>
            <span class="k">continue</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_50_0.png" src="../_images/ModellingFinal_50_0.png" />
</div>
</div>
<p>In beiden Plots ist zu erkennen, dass es deutliche Unterschiede der Klassenmerkmale gibt. Die Interclass Differenz, also die Unterschiede in Merkmalen verschiedener Klassen, ist dementsprechend ausreichend vorhanden. Beispielsweise sind Tucans deutlich an ihrem großen, bunten Schnabel zu erkennen, während Cockatoos einen auffälligen Federschmuck am Kopf besitzen.<br />
Ein Unterschied zwischen den beiden Datasets ist zunächst nicht zu erkennen.</p>
<p>Beim genaueren Untersuchen fällt auch auf, dass die in Notebook 1 erzeugten Bilder unterschiedliche Formate besitzen. Im Gegensatz zu den quadratischen Bildern aus den Datensets von <a class="reference external" href="https://images.cv/">https://images.cv/</a> tauchen hier Bilder in Hochformat und Querformat auf:</p>
<p><strong>selbst erzeugtes Bild in Hochformat:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Laden eines selbst erzeugten Bildes mit Hochformat</span>
<span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">load_img</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;..\ProjectBirdClassification\Bilder1\chicken\chicken (48).png&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_54_0.png" src="../_images/ModellingFinal_54_0.png" />
</div>
</div>
<p><strong>selbst erzeugtes Bild in Querformat:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Laden eines selbst erzeugten Bildes mit Querformat</span>
<span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">load_img</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;..\ProjectBirdClassification\Bilder1\chicken\chicken (51).png&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_56_0.png" src="../_images/ModellingFinal_56_0.png" />
</div>
</div>
<p><strong>Bild von <a class="reference external" href="https://images.cv/">https://images.cv/</a> in quadratischen Format:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Laden eines Bilds von https://images.cv/ in qudratischem Format</span>
<span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">load_img</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;..\ProjectBirdClassification\Bilder3\chicken\0K3D5SDTO0RJ.jpg&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_58_0.png" src="../_images/ModellingFinal_58_0.png" />
</div>
</div>
<p>Beim einlesen solcher Bilder in ein quadratisches Format können somit Verzerrungen entstehen, welche für das Model ein Problem darstellen und ggfs. Einfluss auf die Performance haben könnten. Dies gilt es später zu beachten bzw. zu untersuchen.</p>
<p>Im folgenden Plot sollen 9 Bilder einer Klasse visualisiert werden. Dafür wird in der Variablen <em>plot_class</em> zunächst die ausgewählte Klasse und in <em>class_index</em> deren Index definiert.<br />
Anschließend iterieren wir wieder durch das Dataset und plotten ein Bild nur, wenn das Label der zuvor definierten Klasse entspricht.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_class</span> <span class="o">=</span> <span class="s2">&quot;tucan&quot;</span>
<span class="n">class_index</span> <span class="o">=</span> <span class="n">class_names</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">plot_class</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>
<span class="n">pos</span><span class="o">=</span><span class="mi">0</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_ds3</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">):</span>
      <span class="k">if</span><span class="p">(</span><span class="n">pos</span><span class="o">&lt;</span><span class="mi">9</span><span class="p">):</span>
        <span class="k">if</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">==</span> <span class="n">class_index</span><span class="p">):</span>
            <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">pos</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint8&quot;</span><span class="p">))</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">plot_class</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
            <span class="n">pos</span><span class="o">=</span><span class="n">pos</span><span class="o">+</span><span class="mi">1</span>        
        <span class="k">else</span><span class="p">:</span>
            <span class="k">continue</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">break</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_61_0.png" src="../_images/ModellingFinal_61_0.png" />
</div>
</div>
<p>Die Bilder zeigen den Tucan immer aus anderen Winkeln oder Positionen. Allerdings sind dessen Hauptmerkmale (großer, bunter Schnabel, gelbe Brust, sonst hauptsächliches schwarzes Gefieder) in allen Bildern ausreichend vorhanden. Die Intraclass Varianz, also die Unterschiede innerhalb einer Klasse, sind daher eher klein. Zum Vergleich soll als nächste eine Klasse geplottet werden, welche vermutlich eine höhere Intraclass Varianz aufweist.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_class</span> <span class="o">=</span> <span class="s2">&quot;owl&quot;</span>
<span class="n">class_index</span> <span class="o">=</span> <span class="n">class_names</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">plot_class</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>
<span class="n">pos</span><span class="o">=</span><span class="mi">0</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_ds3</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">):</span>
      <span class="k">if</span><span class="p">(</span><span class="n">pos</span><span class="o">&lt;</span><span class="mi">9</span><span class="p">):</span>
        <span class="k">if</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">==</span> <span class="n">class_index</span><span class="p">):</span>
            <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">pos</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint8&quot;</span><span class="p">))</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">plot_class</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
            <span class="n">pos</span><span class="o">=</span><span class="n">pos</span><span class="o">+</span><span class="mi">1</span>        
        <span class="k">else</span><span class="p">:</span>
            <span class="k">continue</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">break</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_63_0.png" src="../_images/ModellingFinal_63_0.png" />
</div>
</div>
<p>Bei der Klasse Eule können wir erkennen, dass innerhalb der Klasse größere optische Unterschiede vorhanden sind als bei der Klasse Tucan (beispielsweise Schneeeule, Schleiereule, Uhu etc.):</p>
<p><strong>Schleiereule:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">load_img</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;..\ProjectBirdClassification\Bilder3\owl\0JQ0CI04VUGY.jpg&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_66_0.png" src="../_images/ModellingFinal_66_0.png" />
</div>
</div>
<p><strong>Schneeeule:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">load_img</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;..\ProjectBirdClassification\Bilder3\owl\0NL38ZA7X622.jpg&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_68_0.png" src="../_images/ModellingFinal_68_0.png" />
</div>
</div>
<p><strong>Uhu:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">load_img</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;..\ProjectBirdClassification\Bilder3\owl\0SKMABBW4H2K.jpg&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_70_0.png" src="../_images/ModellingFinal_70_0.png" />
</div>
</div>
<p>Dies könnte bedeuten, dass die richtige Klassifikation dieser Klasse für das Modell schwieriger ist und somit schlechtere Ergebnisse liefert.<br />
Die soll später bei der Evaluation der Modelle überprüft werden.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="image-augmentation">
<h1>5. Image Augmentation<a class="headerlink" href="#image-augmentation" title="Permalink to this headline">#</a></h1>
<p>Um mit einer Bildklassifikation mithilfe von Neuronalen Netzen gute Ergebnisse zu erzielen, ist eine große Menge an Trainingsdaten notwendig. Nur so kann das Modell ausreichend trainiert werden, um die Merkmale unterschiedlicher Klassen gut zu unterscheiden.<br />
Liegen nicht genügend Trainingsdaten vor, kann durch Image Augmentation die Datenmenge künstlich vergrößert werden.<br />
Dafür werden vorhandene Bilder zufällig gedreht, gespiegelt und vergrößert bzw. verkleinert und diese neu generierten Bilder dem Modell als neue Bilder zur Verfügung zu stellen.</p>
<p>Keras stellt hierfür verschiedene Funktionen zur Verfügung. Mit <em>RandomFlip</em> wird ein Bild zufällig (auswählbar ob horizontal, vertikal oder beides) gespiegelt, während <em>RandomRotation</em> ein Bild um einen gewissen Faktor zufällig dreht. Zusätzlich existiert beispielsweise auch der Layer <em>RandomZoom</em>, unter dessen Verwendung ein Bild zufällig vergrößert wird.</p>
<p>Nachfolgend demonstrieren wir die Methoden <em>RandomFlip</em> und <em>RandomRotation</em> mit unserem Datenset.<br />
Hierfür können die beiden Preprocessing Layer in einem Sequential Objekt kombiniert werden, sodass diese sich wie eine Funktion mit dem Namen <em>data_augmentation</em> aufrufen lassen.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_augmentation</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">RandomFlip</span><span class="p">(</span><span class="s2">&quot;horizontal_and_vertical&quot;</span><span class="p">),</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">RandomRotation</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Das Ergebnis dieser Augmentation visualisieren wir wieder in einem Plot, indem wir durch das Datenset iterieren und zunächst das Originalbild und anschließend das augmentierte Bild anzeigen. Die Augmentation erfolgt, indem die Originalbilder <em>images</em> an die eben erzeugte Funktionalität <em>data_augmentation</em> übergeben werden. Dadurch wird jedes Originalbild einmal indivduell zufällig gespiegelt und rotiert.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_ds1</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">augmented_images</span> <span class="o">=</span> <span class="n">data_augmentation</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
        <span class="n">axarr</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint8&quot;</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;original: &quot;</span> <span class="o">+</span> <span class="n">class_names</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)])</span>
        <span class="n">axarr</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">augmented_images</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint8&quot;</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;augmented: &quot;</span> <span class="o">+</span> <span class="n">class_names</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.
</pre></div>
</div>
<img alt="../_images/ModellingFinal_77_1.png" src="../_images/ModellingFinal_77_1.png" />
</div>
</div>
<p>Es ist zu erkennen, dass jedes Orignalbild mit einem anderen Faktor gedreht wurde und auch jedes Originalbild anders gespiegelt wurde.</p>
<p>Die Image Augmentation kann entweder zu Beginn auf das Datenset angewendet werden und deren Ergebnisse persistiert und dem Datenset hinzugefügt werden. Somit vergrößert sich natürlich die Datenmenge. In einer Trainingsepoche des Neuronalen Netzes stehen mehr Bilder zur Verfügung, als wenn nur Originalbilder verwendet werden.<br />
Alternativ besteht die Möglichkeit, die eben demonstrierten Preprocessing Layer der Architektur eines Neuronalen Netzes hinzuzufügen. Somit wird in der Trainingsphase jedes Bild zunächst augmentiert, bevor es den Rest des Modells durchläuft. Dadurch vergrößert sich zwar nicht die Datenmenge innerhalb einer Modellepoche, da jedes Bild genau einmal das neuronale Netz durchläuft. In der nächsten Epoche wird das Bild aber wieder augmentiert, um einen zufälligen Faktor. Somit erscheint es für das Modell als neues Bild.<br />
Vergrößern wir also die Anzahl der Epochen, im Vergleich zu Alternative 1, so erzielen wir auf lange Sicht den gleichen Effekt und vergrößern unser Datenset. Für diese Alternative haben wir uns in unserem Projekt entschieden, weshalb die Ergebnisse der demonstrierten Image Augmentation nicht gespeichert werden. Stattdessen fügen wir die beiden Preprocessing Layer <em>RandomFlip</em> und <em>RandomRotation</em> unseren Modellen hinzu.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="building-training-a-cnn-model-from-scratch">
<h1>6. Building &amp; Training a CNN Model from Scratch<a class="headerlink" href="#building-training-a-cnn-model-from-scratch" title="Permalink to this headline">#</a></h1>
<p>Da sich wie bereits erwähnt CNN am besten zur Multiclass Classification von Bildern eignen, sollen in diesem Kapitel verschiedene CNN Architekturen selbst erstellt und getestet werden. Durch das Designen einer geeigneten Architektur sowie das Auswählen optimaler Parameter soll eine möglichst hohe Accuracy erzielt werden.<br />
Wir beginnen mit einem simplen Modell und steigern die Komplexität der Architektur &amp; Parameter von Modell zu Modell.</p>
<section id="model-1-simples-cnn">
<h2>6.1 Model 1 - simples CNN<a class="headerlink" href="#model-1-simples-cnn" title="Permalink to this headline">#</a></h2>
<section id="a-model-1-introduction">
<h3>a) Model 1 - Introduction<a class="headerlink" href="#a-model-1-introduction" title="Permalink to this headline">#</a></h3>
<p>In diesem ersten Versuch soll ein simples CNN mit wenigen Layern und wenigen Filtern erstellt werden. Dies soll als Test dienen, welche Ergebnisse mit einem einfachen Netz und wenig Ressourcen erzielt werden können.<br />
Hier soll der Fokus noch nicht auf der Optimierung der Architektur und der Hyperparameter liegen. Dies erfolgt später in Netzen mit komplexerer Architektur, welche auch mehr Aussicht auf Erfolg bieten.<br />
Außerdem bietet sich dieses Netz an, um die drei unterschiedlichen Datasets durchzutesten und zu vergleichen, da das CNN auf Grund der einfachen Architektur vergleichsweise schnell trainiert werden kann.</p>
</section>
<section id="b-model-1-construction">
<h3>b) Model 1 - Construction<a class="headerlink" href="#b-model-1-construction" title="Permalink to this headline">#</a></h3>
<p>Um möglichst einfach mehrere gleiche Modelle zum Testen der verschiedenen Datasets erstellen zu können, wird die Konstruktion des Modells in eine Funktion ausgelagert, welche immer wieder aufgerufen werden kann um ein gleiches CNN zu erstellen.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">construct_model1</span><span class="p">():</span>
    <span class="n">cnn1</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    <span class="n">cnn1</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">cnn1</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
    <span class="n">cnn1</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">))</span>
    <span class="n">cnn1</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPool2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
    <span class="n">cnn1</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
    <span class="n">cnn1</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">cnn1</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
    <span class="n">cnn1</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">cnn1</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Architektur und Parameter:</strong><br />
An dieser Stelle wurde nicht viel Wert auf optimale Architektur oder Parameter gelegt. Layer, Filter und weitere Parameter wurden lediglich so gewählt, dass das Training des Modells einigermaßen schnell auf lokaler Hardware durchführbar ist um möglichst schnell ein erstes Gefühl für die Verwertbarkeit der verschiedenen Datasets zu bekommen sowie für den Accuracy Wert eines nicht optimierten, simplen CNN.<br />
Die Bilder zum Trainieren wurden für diese Modelle mit einer Auflösung von 64x64 Pixel eingelesen, da nach Tests keine Auswirkung der Auflösung im Gegensatz zu 128x128 festgestellt werden konnte und mit 64x64 Pixel das Modell schneller trainiert werden kann.</p>
<p>Als nächstes werden nun drei Modelle erstellt, welche mit den drei unterschiedlichen datasets trainiert werden sollen, um Unterschiede zwischen den datasets zu untersuchen.<br />
Das erste Modell soll mit Dataset 1 trainiert werden, das zweite Modell mit Dataset 2 und das dritte Modell mit Dataset 3.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Erstellen des ersten simplen CNN Modells</span>
<span class="n">model1_1</span><span class="o">=</span><span class="n">construct_model1</span><span class="p">()</span>
<span class="n">model1_1</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;Adam&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Erstellen des zweiten simplen CNN Modells</span>
<span class="n">model1_2</span><span class="o">=</span><span class="n">construct_model1</span><span class="p">()</span>
<span class="n">model1_2</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;Adam&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Erstellen des dritten simplen CNN Modells</span>
<span class="n">model1_3</span><span class="o">=</span><span class="n">construct_model1</span><span class="p">()</span>
<span class="n">model1_3</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;Adam&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Bevor die Modelle trainiert werden, können wir die Layer unserer simplen CNN Architektur mit Hilfe einer visualkeras util visualisieren:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visuelle Darstellung der Layer von Modell 1, gleich für alle drei erstellten Modelle</span>
<span class="n">visualkeras</span><span class="o">.</span><span class="n">layered_view</span><span class="p">(</span><span class="n">model1_1</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_94_0.png" src="../_images/ModellingFinal_94_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model1_1</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_11&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_38 (Conv2D)          (None, 64, 64, 32)        896       
                                                                 
 dropout_14 (Dropout)        (None, 64, 64, 32)        0         
                                                                 
 conv2d_39 (Conv2D)          (None, 64, 64, 32)        9248      
                                                                 
 max_pooling2d_19 (MaxPoolin  (None, 32, 32, 32)       0         
 g2D)                                                            
                                                                 
 flatten_10 (Flatten)        (None, 32768)             0         
                                                                 
 dense_22 (Dense)            (None, 512)               16777728  
                                                                 
 dropout_15 (Dropout)        (None, 512)               0         
                                                                 
 dense_23 (Dense)            (None, 9)                 4617      
                                                                 
=================================================================
Total params: 16,792,489
Trainable params: 16,792,489
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
</section>
<section id="c-model-1-training">
<h3>c) Model 1 - Training<a class="headerlink" href="#c-model-1-training" title="Permalink to this headline">#</a></h3>
<p>Nun werden die zuvor erstellten Modelle mit den jeweiligen Datasets trainiert.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Trainieren des einfachen CNN mit dataset 1 (in Notebook 1 erzeugte Bilder)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Trainieren des einfachen CNN mit dataset 1 (in Notebook 1 erzeugte Bilder)&quot;</span><span class="p">)</span>
<span class="n">history1_1</span> <span class="o">=</span> <span class="n">model1_1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_ds1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">val_ds1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Trainieren des einfachen CNN mit dataset 1 (in Notebook 1 erzeugte Bilder)
Epoch 1/12
142/142 [==============================] - 26s 179ms/step - loss: 111.1921 - accuracy: 0.1181 - val_loss: 2.1943 - val_accuracy: 0.1249
Epoch 2/12
142/142 [==============================] - 26s 183ms/step - loss: 2.1762 - accuracy: 0.1423 - val_loss: 2.2005 - val_accuracy: 0.1196
Epoch 3/12
142/142 [==============================] - 26s 182ms/step - loss: 2.0975 - accuracy: 0.2049 - val_loss: 2.2505 - val_accuracy: 0.1534
Epoch 4/12
142/142 [==============================] - 26s 185ms/step - loss: 1.9762 - accuracy: 0.2664 - val_loss: 2.3837 - val_accuracy: 0.1672
Epoch 5/12
142/142 [==============================] - 26s 182ms/step - loss: 1.8399 - accuracy: 0.3239 - val_loss: 2.5809 - val_accuracy: 0.1799
Epoch 6/12
142/142 [==============================] - 25s 176ms/step - loss: 1.7185 - accuracy: 0.3743 - val_loss: 2.7120 - val_accuracy: 0.1746
Epoch 7/12
142/142 [==============================] - 26s 182ms/step - loss: 1.5944 - accuracy: 0.4155 - val_loss: 2.8008 - val_accuracy: 0.1788
Epoch 8/12
142/142 [==============================] - 26s 186ms/step - loss: 1.4786 - accuracy: 0.4657 - val_loss: 3.1066 - val_accuracy: 0.2095
Epoch 9/12
142/142 [==============================] - 26s 183ms/step - loss: 1.4072 - accuracy: 0.4929 - val_loss: 3.3511 - val_accuracy: 0.1979
Epoch 10/12
142/142 [==============================] - 26s 183ms/step - loss: 1.3187 - accuracy: 0.5334 - val_loss: 3.6567 - val_accuracy: 0.2148
Epoch 11/12
142/142 [==============================] - 27s 188ms/step - loss: 1.1975 - accuracy: 0.5666 - val_loss: 3.7067 - val_accuracy: 0.2042
Epoch 12/12
142/142 [==============================] - 26s 185ms/step - loss: 1.1345 - accuracy: 0.5989 - val_loss: 4.0633 - val_accuracy: 0.2360
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Trainieren des einfachen CNN mit dataset 2 (in Notebook 1 erzeugte Bilder + Bilder von https://images.cv/)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Trainieren des einfachen CNN mit dataset 2 (in Notebook 1 erzeugte Bilder + Bilder von https://images.cv/)&quot;</span><span class="p">)</span>
<span class="n">history1_2</span> <span class="o">=</span> <span class="n">model1_2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_ds2</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">val_ds2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Trainieren des einfachen CNN mit dataset 2 (in Notebook 1 erzeugte Bilder + Bilder von https://images.cv/)
Epoch 1/12
318/318 [==============================] - 57s 179ms/step - loss: 25.5834 - accuracy: 0.1824 - val_loss: 2.1637 - val_accuracy: 0.1812
Epoch 2/12
318/318 [==============================] - 57s 178ms/step - loss: 2.0812 - accuracy: 0.2289 - val_loss: 2.1538 - val_accuracy: 0.1780
Epoch 3/12
318/318 [==============================] - 57s 178ms/step - loss: 1.9549 - accuracy: 0.2826 - val_loss: 2.2053 - val_accuracy: 0.2194
Epoch 4/12
318/318 [==============================] - 57s 178ms/step - loss: 1.8038 - accuracy: 0.3486 - val_loss: 2.3822 - val_accuracy: 0.2222
Epoch 5/12
318/318 [==============================] - 57s 179ms/step - loss: 1.6596 - accuracy: 0.4078 - val_loss: 2.4791 - val_accuracy: 0.2282
Epoch 6/12
318/318 [==============================] - 57s 179ms/step - loss: 1.4568 - accuracy: 0.4792 - val_loss: 2.4967 - val_accuracy: 0.2534
Epoch 7/12
318/318 [==============================] - 61s 191ms/step - loss: 1.3180 - accuracy: 0.5354 - val_loss: 2.6902 - val_accuracy: 0.2824
Epoch 8/12
318/318 [==============================] - 60s 189ms/step - loss: 1.1553 - accuracy: 0.5983 - val_loss: 2.8420 - val_accuracy: 0.3059
Epoch 9/12
318/318 [==============================] - 62s 195ms/step - loss: 1.0027 - accuracy: 0.6577 - val_loss: 2.9292 - val_accuracy: 0.3307
Epoch 10/12
318/318 [==============================] - 65s 205ms/step - loss: 0.8452 - accuracy: 0.7143 - val_loss: 3.2722 - val_accuracy: 0.3500
Epoch 11/12
318/318 [==============================] - 64s 201ms/step - loss: 0.7246 - accuracy: 0.7606 - val_loss: 3.3062 - val_accuracy: 0.3818
Epoch 12/12
318/318 [==============================] - 68s 212ms/step - loss: 0.6521 - accuracy: 0.7863 - val_loss: 3.1865 - val_accuracy: 0.3776
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Trainieren des einfachen CNN mit dataset 3 (nur Bilder von https://images.cv/)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Trainieren des einfachen CNN mit dataset 3 (nur Bilder von https://images.cv/)&quot;</span><span class="p">)</span>
<span class="n">history1_3</span> <span class="o">=</span> <span class="n">model1_3</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_ds3</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">val_ds3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Trainieren des einfachen CNN mit dataset 3 (nur Bilder von https://images.cv/)
Epoch 1/12
212/212 [==============================] - 43s 199ms/step - loss: 87.1206 - accuracy: 0.1785 - val_loss: 2.1573 - val_accuracy: 0.1913
Epoch 2/12
212/212 [==============================] - 41s 192ms/step - loss: 2.0826 - accuracy: 0.2316 - val_loss: 2.1355 - val_accuracy: 0.2004
Epoch 3/12
212/212 [==============================] - 41s 193ms/step - loss: 1.9609 - accuracy: 0.2868 - val_loss: 2.1087 - val_accuracy: 0.2207
Epoch 4/12
212/212 [==============================] - 42s 197ms/step - loss: 1.8132 - accuracy: 0.3464 - val_loss: 2.1459 - val_accuracy: 0.2605
Epoch 5/12
212/212 [==============================] - 42s 199ms/step - loss: 1.6763 - accuracy: 0.3944 - val_loss: 2.1253 - val_accuracy: 0.2919
Epoch 6/12
212/212 [==============================] - 42s 199ms/step - loss: 1.5111 - accuracy: 0.4717 - val_loss: 2.3439 - val_accuracy: 0.2926
Epoch 7/12
212/212 [==============================] - 42s 197ms/step - loss: 1.3859 - accuracy: 0.5192 - val_loss: 2.2750 - val_accuracy: 0.2863
Epoch 8/12
212/212 [==============================] - 42s 196ms/step - loss: 1.1999 - accuracy: 0.5845 - val_loss: 2.6009 - val_accuracy: 0.3233
Epoch 9/12
212/212 [==============================] - 41s 193ms/step - loss: 1.0795 - accuracy: 0.6326 - val_loss: 2.7088 - val_accuracy: 0.3268
Epoch 10/12
212/212 [==============================] - 42s 196ms/step - loss: 0.9636 - accuracy: 0.6781 - val_loss: 2.8338 - val_accuracy: 0.3624
Epoch 11/12
212/212 [==============================] - 42s 198ms/step - loss: 0.8418 - accuracy: 0.7207 - val_loss: 3.4049 - val_accuracy: 0.3820
Epoch 12/12
212/212 [==============================] - 42s 198ms/step - loss: 0.7424 - accuracy: 0.7555 - val_loss: 3.6743 - val_accuracy: 0.3743
</pre></div>
</div>
</div>
</div>
<p>Am Trainingsverlauf können wir bereits erkennen, dass sich Dataset 2 und Dataset 3 am besten als Input für das Modell eignen.</p>
</section>
<section id="d-model-1-evaluation">
<h3>d) Model 1 Evaluation<a class="headerlink" href="#d-model-1-evaluation" title="Permalink to this headline">#</a></h3>
<p>Für die Evaluation der Modelle ist zunächst deren Trainings- und Validationaccuracy sowie der Loss interessant.<br />
Hierfür wird die Funktion <em>plot_accuracy</em> definiert, welche den Verlauf dieser beiden Werte für Trainings- und Validierungsdaten über alle Epochen visualisiert. Als Input benötigt diese Funktion die Modelhistory, welche in der Trainingsphase des jeweiligen Modells erstellt wurde. Aus dieser Historie können die einzelnen Epochen Werte von Accuracy und Loss sowie deren Maximum bzw. Minimum bestimmt und anschließend geplotted werden.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Erstellen einer Funktion zum plotten der Accuracy</span>
<span class="k">def</span> <span class="nf">plot_accuracy</span><span class="p">(</span><span class="n">history</span><span class="p">):</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_accuracy&#39;</span><span class="p">]</span>
    <span class="n">max_val_acc</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">val_acc</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>
    <span class="n">val_loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">]</span>
    <span class="n">min_val_loss</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span>

    <span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="s1">&#39;bo-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training accuracy&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="s1">&#39;ro-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation accuracy&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training and validation accuracy&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Maximum accuracy: &quot;</span><span class="p">,</span><span class="n">max_val_acc</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Minimum Loss: &quot;</span><span class="p">,</span><span class="n">min_val_loss</span><span class="p">)</span>



    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;bo-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="s1">&#39;ro-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training and validation loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy und Loss des simplen CNN mit dataset 1:&quot;</span><span class="p">)</span>
<span class="n">plot_accuracy</span><span class="p">(</span><span class="n">history1_1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy und Loss des simplen CNN mit dataset 2:&quot;</span><span class="p">)</span>
<span class="n">plot_accuracy</span><span class="p">(</span><span class="n">history1_2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy und Loss des simplen CNN mit dataset 3:&quot;</span><span class="p">)</span>
<span class="n">plot_accuracy</span><span class="p">(</span><span class="n">history1_3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy und Loss des simplen CNN mit dataset 1:
Maximum accuracy:  0.2359788417816162
Minimum Loss:  2.1943488121032715
</pre></div>
</div>
<img alt="../_images/ModellingFinal_105_1.png" src="../_images/ModellingFinal_105_1.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy und Loss des simplen CNN mit dataset 2:
Maximum accuracy:  0.38178473711013794
Minimum Loss:  2.1537678241729736
</pre></div>
</div>
<img alt="../_images/ModellingFinal_105_3.png" src="../_images/ModellingFinal_105_3.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy und Loss des simplen CNN mit dataset 3:
Maximum accuracy:  0.3819832503795624
Minimum Loss:  2.1086957454681396
</pre></div>
</div>
<img alt="../_images/ModellingFinal_105_5.png" src="../_images/ModellingFinal_105_5.png" />
</div>
</div>
<p>Wie bereits zuvor schon vermutet, lässt sich an Hand der hier visualisierten Metriken noch einmal bestätigen, dass die Datasets 2 und 3 beim gleichen Modell eine bessere Performance liefern als Dataset 1. Der Trainingsverlauf an sich scheint einigermaßen stabil, die Validation Accuracy steigt von Epoche zu Epoche meist leicht an. Vermutlich hätte durch weitere Epochen ein noch besseres Ergebnis erzielt werden können. Da dieses Modell allerdings nur ein erster Test sein sollte, belassen wir es bei dem oben dargestellten Training.</p>
<p>Mit Accuracy und Loss alleine kann, wie zu Beginn des Notebooks beschrieben, keine ausreichende Aussage über Performance eines Modells getroffen werden. Daher wollen wir nun auch Precision, Recall und F1-Score der Modelle betrachten.<br />
Hierfür definieren wir zunächst die Funktion <em>predict_testdata</em>, die für das übergebene Modell und Testdaten y_true und y_pred bestimmt. Dafür iteriert die Funktion über die Testdaten und bestimmt für jeden Batch die jeweils richtige und die durch das Modell vorhergesagte Klasse. Zuletzt werden die einzelnen Batchelemente von y_true und y_pred in jeweils ein Tensorobjekt konvertiert und dieses als Output der Funktion zurückgegeben.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Funktion berechnet y_pred und y_true für ein Modell, daher Modell und richtiges test_ds mitgeben, als output kommt y_true und y_pred</span>
<span class="k">def</span> <span class="nf">predict_testdata</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_ds</span><span class="p">):</span>
    
   <span class="n">y_pred</span> <span class="o">=</span> <span class="p">[]</span>  
   <span class="n">y_true</span> <span class="o">=</span> <span class="p">[]</span>  

   <span class="k">for</span> <span class="n">image_batch</span><span class="p">,</span> <span class="n">label_batch</span> <span class="ow">in</span> <span class="n">test_ds</span><span class="p">:</span>   
      <span class="n">y_true</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">label_batch</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
      <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">image_batch</span><span class="p">)</span>
      <span class="n">y_pred</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>

   <span class="c1"># Konvertierung zu Tensor Objekten</span>
   <span class="n">correct_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">item</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">y_true</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
   <span class="n">predicted_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">item</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">y_pred</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

   <span class="k">return</span> <span class="n">correct_labels</span><span class="p">,</span> <span class="n">predicted_labels</span>
</pre></div>
</div>
</div>
</div>
<p>Diese Funktion wird nun beispielhaft für das beste der oben trainierten Modelle (model1_3) und test_ds3 aufgerufen und die beiden Ergebnisse in den Variablen model1_3_corL (für correctLabel) und model1_3_predL (für predictedLabel) gespeichert.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Aufruf der Funktion</span>
<span class="n">model1_3_corL</span><span class="p">,</span> <span class="n">model1_3_predL</span> <span class="o">=</span> <span class="n">predict_testdata</span><span class="p">(</span><span class="n">model1_3</span><span class="p">,</span> <span class="n">test_ds3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1/1 [==============================] - 0s 84ms/step
1/1 [==============================] - 0s 47ms/step
1/1 [==============================] - 0s 44ms/step
1/1 [==============================] - 0s 46ms/step
1/1 [==============================] - 0s 48ms/step
1/1 [==============================] - 0s 43ms/step
1/1 [==============================] - 0s 46ms/step
1/1 [==============================] - 0s 42ms/step
1/1 [==============================] - 0s 44ms/step
1/1 [==============================] - 0s 43ms/step
1/1 [==============================] - 0s 42ms/step
1/1 [==============================] - 0s 45ms/step
1/1 [==============================] - 0s 42ms/step
1/1 [==============================] - 0s 40ms/step
1/1 [==============================] - 0s 41ms/step
1/1 [==============================] - 0s 44ms/step
1/1 [==============================] - 0s 44ms/step
1/1 [==============================] - 0s 41ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 41ms/step
1/1 [==============================] - 0s 43ms/step
1/1 [==============================] - 0s 38ms/step
1/1 [==============================] - 0s 38ms/step
1/1 [==============================] - 0s 38ms/step
1/1 [==============================] - 0s 40ms/step
1/1 [==============================] - 0s 42ms/step
1/1 [==============================] - 0s 41ms/step
1/1 [==============================] - 0s 40ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 41ms/step
1/1 [==============================] - 0s 43ms/step
1/1 [==============================] - 0s 40ms/step
1/1 [==============================] - 0s 40ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 44ms/step
1/1 [==============================] - 0s 43ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 41ms/step
1/1 [==============================] - 0s 40ms/step
1/1 [==============================] - 0s 41ms/step
1/1 [==============================] - 0s 41ms/step
1/1 [==============================] - 0s 42ms/step
1/1 [==============================] - 0s 37ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 43ms/step
</pre></div>
</div>
</div>
</div>
<p>Mithilfe der eben erzeugten Variablen können die Metriken Precision, Recall und F1-Score nun für das Modell angezeigt werden.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">model1_3_corL</span><span class="p">,</span> <span class="n">model1_3_predL</span><span class="p">,</span><span class="n">target_names</span> <span class="o">=</span> <span class="n">class_names</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>              precision    recall  f1-score   support

     chicken       0.35      0.35      0.35       180
    cockatoo       0.31      0.30      0.30       138
       eagle       0.40      0.31      0.35       129
    flamingo       0.37      0.42      0.40       160
     ostrich       0.37      0.16      0.23       165
         owl       0.19      0.20      0.19       121
     penguin       0.30      0.41      0.35       192
         tit       0.43      0.40      0.41       197
       tucan       0.39      0.47      0.43       190

    accuracy                           0.35      1472
   macro avg       0.34      0.34      0.33      1472
weighted avg       0.35      0.35      0.34      1472
</pre></div>
</div>
</div>
</div>
<p>Anschließend wird die Confusion Matrix des Modells mithilfe der importieren Funktion <em>plot_confusion_matrix</em> erstellt.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">model1_3_corL</span><span class="p">,</span> <span class="n">model1_3_predL</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_114_0.png" src="../_images/ModellingFinal_114_0.png" />
</div>
</div>
<p>An Classification Report und Confusion Matrix lässt sich leicht erkennen, dass das Modell 1 nicht in der Lage ist korrekte Klassifizierungen durchzuführen. Vor allem die Klasse Owl wird nur sehr schlecht erkannt, und viele Bilder werden falsch als Flamingo klassifiziert.</p>
</section>
<section id="e-model-1-fazit">
<h3>e) Model 1 - Fazit<a class="headerlink" href="#e-model-1-fazit" title="Permalink to this headline">#</a></h3>
<p>In Modell 1 können wir bereits erkennen, dass sich Dataset 1 nicht besonders gut eignet. Daher sollen in den weiteren Modellen nurnoch Dataset 2 und Dataset 3 verwendet werden. Dataset 1 beinhaltet vermutlich zu wenige Bilder in guter Qualität. Außerdem könnte das zuvor erwähnte Problem mit Hoch- und Querformat der originalen Bilder eine Rolle spielen.</p>
<p>Die Performance von Modell 1 ist noch nicht optimal, der Fokus auf Optimierung der Architektur und Parameter erfolgt allerdings auch erst in Modell 2.</p>
</section>
</section>
<section id="model-2-komplexeres-cnn">
<h2>6.2 Model 2 - komplexeres CNN<a class="headerlink" href="#model-2-komplexeres-cnn" title="Permalink to this headline">#</a></h2>
<section id="a-model-2-introduction">
<h3>a) Model 2 - Introduction<a class="headerlink" href="#a-model-2-introduction" title="Permalink to this headline">#</a></h3>
<p>Mit den bei Modell 1 gewonnen Erkentnissen soll nun ein zweites Modell designed werden, welches mehr Layer und Filter besitzt als das vorherige und damit auch bessere Ergebnisse erzielt. Der Anspruch ist es, hierbei eine Konfiguration zu finden, welche sich in angemessener Zeit auf der lokalen Hardware trainieren lässt.</p>
</section>
<section id="b-model-2-construction">
<h3>b) Model 2 - Construction<a class="headerlink" href="#b-model-2-construction" title="Permalink to this headline">#</a></h3>
<p>Wie bereits in Modell 1 wird das Design des Modells in eine Funktion ausgelagert, um schnell mehrere Modelle erstellen und testen zu können. Zusätzlich wird hier noch die Learning Rate in die Funktion als Parameter mitgegeben, um diese beim Aufrufen der Funktion zu Testzwecken einfach ändern zu können.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">construct_cnn2</span><span class="p">(</span><span class="n">learningRate</span><span class="p">):</span>
    <span class="n">cnn2</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    <span class="n">cnn2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">RandomFlip</span><span class="p">(</span><span class="s2">&quot;horizontal_and_vertical&quot;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span><span class="mi">3</span><span class="p">)))</span>
    <span class="n">cnn2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">RandomRotation</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
    <span class="n">cnn2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">cnn2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">))</span>
    <span class="n">cnn2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">cnn2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">))</span>
    <span class="n">cnn2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPool2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
    <span class="n">cnn2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">cnn2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">))</span>
    <span class="n">cnn2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">cnn2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">))</span>
    <span class="n">cnn2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPool2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
    <span class="n">cnn2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">cnn2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">))</span>
    <span class="n">cnn2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">cnn2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">))</span>
    <span class="n">cnn2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPool2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
    <span class="n">cnn2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
    <span class="n">cnn2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">cnn2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">cnn2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">learningRate</span><span class="p">)</span>
    <span class="n">cnn2</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">cnn2</span>
</pre></div>
</div>
</div>
</div>
<p>In diesem Modell wurden viele Parameter und Architekturen getestet:</p>
<p><strong>Architektur und Parameter:</strong><br />
<strong>Batch Size:</strong> 32 hat sich durch Tests als effizient erwiesen<br />
<strong>Auflösung der Bilder</strong>: 32x32 Pixel erzielte weniger gute Ergebnisse, zwischen 64x64 und 128x128 konnten in mehreren Probedurchläufen keine ausschlaggebenden Unterschiede festegestellt werden. Aus Performance Gründen wurde daher 64x64 Pixel gewählt.<br />
<strong>Anzahl Layer:</strong>  Bei der Anzahl an Layer wurde durch Tests versucht, ein Optimum zwischen Geschwindigkeit und Ergebnis zu finden. Zur Orientierung für den Aufbau der Layer wurden best practice Architekturen aus dem Internet, bspw. GitHub herangezogen.<br />
<strong>Anzahl Filter:</strong> Hier wurde als best practice eine von Layer zu Layer aufsteigende Anzahl an Filtern getestet zwischen 32 und 256. Die beste Ergebnisse wurden wie oben dargestellt erzielt.<br />
<strong>Filtergröße:</strong> 3x3 als Standard bei Bildern kleiner 128x128<br />
<strong>Dropout:</strong> ein Dropout mit geringer Größe nach jedem convolutional Layer hat sich als wirkungsvoll erwiesen, empfohlen aus Forschung/Literatur (<a class="reference external" href="http://mipal.snu.ac.kr/images/1/16/Dropout_ACCV2016.pdf">http://mipal.snu.ac.kr/images/1/16/Dropout_ACCV2016.pdf</a>)<br>
<strong>Augmentation:</strong>  Mit den Keras Layern Random Flip und Random Rotation konnten etwas bessere Ergebnisse erzielt werden.<br />
<strong>Activation:</strong> Als Aktivierungsfunktion für die Hidden Layer wurden relu und tanh als CNN Standards getestet, mit relu konnten die besseren Ergebnisse erzielt werden. Für den Ausgabe Layer wurde softmax als Standard gewählt.<br />
<strong>Optimizer:</strong>  Adam und SGD wurden getestet, wobei keine merkbaren Unterschiede aufgefallen sind, daher wurde Adam gewählt.<br />
<strong>Learning Rate:</strong> Learning Rate von 0,001 bewährt, liefert ein “stabileres” Training mit weniger Schwankungen in Validation Accuracy als bspw. 0,01<br />
<strong>Loss:</strong> Categorical Crossentropy als Standard für multiclass classification CNN.</p>
<p>Da sich Dataset 1 als weniger geeignet erwiesen hat, werden nun nurnoch zwei gleiche Modelle erstellt, welche mit Dataset 2 und 3 trainiert werden sollen:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Erstellen des ersten komplexeren CNN Modells, welches mit Dataset 2 trainiert werden soll:</span>
<span class="n">Model2_1</span><span class="o">=</span><span class="n">construct_cnn2</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">Model2_1</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;Adam&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Erstellen des zweiten komplexeren CNN Modells, welches mit Dataset 3 trainiert werden soll:</span>
<span class="n">Model2_2</span><span class="o">=</span><span class="n">construct_cnn2</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">Model2_2</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;Adam&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Auch lässt sich gut erkennen, dass Modell 2 aus mehr Schichten besteht als Modell 1:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visuelle Darstellung der Layer von Modell 2, gleich für alle zwei erstellten Modelle</span>
<span class="n">visualkeras</span><span class="o">.</span><span class="n">layered_view</span><span class="p">(</span><span class="n">Model2_1</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_130_0.png" src="../_images/ModellingFinal_130_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Model2_1</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_57&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 random_flip_43 (RandomFlip)  (None, 64, 64, 3)        0         
                                                                 
 random_rotation_40 (RandomR  (None, 64, 64, 3)        0         
 otation)                                                        
                                                                 
 conv2d_294 (Conv2D)         (None, 62, 62, 32)        896       
                                                                 
 dropout_128 (Dropout)       (None, 62, 62, 32)        0         
                                                                 
 conv2d_295 (Conv2D)         (None, 60, 60, 32)        9248      
                                                                 
 dropout_129 (Dropout)       (None, 60, 60, 32)        0         
                                                                 
 max_pooling2d_137 (MaxPooli  (None, 30, 30, 32)       0         
 ng2D)                                                           
                                                                 
 conv2d_296 (Conv2D)         (None, 28, 28, 64)        18496     
                                                                 
 dropout_130 (Dropout)       (None, 28, 28, 64)        0         
                                                                 
 conv2d_297 (Conv2D)         (None, 26, 26, 64)        36928     
                                                                 
 dropout_131 (Dropout)       (None, 26, 26, 64)        0         
                                                                 
 max_pooling2d_138 (MaxPooli  (None, 13, 13, 64)       0         
 ng2D)                                                           
                                                                 
 conv2d_298 (Conv2D)         (None, 11, 11, 128)       73856     
                                                                 
 dropout_132 (Dropout)       (None, 11, 11, 128)       0         
                                                                 
 conv2d_299 (Conv2D)         (None, 9, 9, 128)         147584    
                                                                 
 dropout_133 (Dropout)       (None, 9, 9, 128)         0         
                                                                 
 max_pooling2d_139 (MaxPooli  (None, 4, 4, 128)        0         
 ng2D)                                                           
                                                                 
 flatten_49 (Flatten)        (None, 2048)              0         
                                                                 
 dense_128 (Dense)           (None, 256)               524544    
                                                                 
 dense_129 (Dense)           (None, 256)               65792     
                                                                 
 dense_130 (Dense)           (None, 9)                 2313      
                                                                 
=================================================================
Total params: 879,657
Trainable params: 879,657
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
</section>
<section id="c-model-2-training">
<h3>c) Model 2 - Training<a class="headerlink" href="#c-model-2-training" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history2_1</span> <span class="o">=</span> <span class="n">Model2_1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_ds2</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">val_ds2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/40
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.
318/318 [==============================] - 109s 337ms/step - loss: 2.2013 - accuracy: 0.2817 - val_loss: 1.8625 - val_accuracy: 0.3027
Epoch 2/40
318/318 [==============================] - 102s 320ms/step - loss: 1.7120 - accuracy: 0.3874 - val_loss: 1.6717 - val_accuracy: 0.4172
Epoch 3/40
318/318 [==============================] - 104s 327ms/step - loss: 1.5720 - accuracy: 0.4508 - val_loss: 1.4360 - val_accuracy: 0.5064
Epoch 4/40
318/318 [==============================] - 102s 320ms/step - loss: 1.4376 - accuracy: 0.5060 - val_loss: 1.4346 - val_accuracy: 0.5041
Epoch 5/40
318/318 [==============================] - 101s 318ms/step - loss: 1.3560 - accuracy: 0.5320 - val_loss: 1.3100 - val_accuracy: 0.5672
Epoch 6/40
318/318 [==============================] - 100s 314ms/step - loss: 1.3076 - accuracy: 0.5524 - val_loss: 1.2158 - val_accuracy: 0.5938
Epoch 7/40
318/318 [==============================] - 104s 328ms/step - loss: 1.2364 - accuracy: 0.5803 - val_loss: 1.1598 - val_accuracy: 0.6237
Epoch 8/40
318/318 [==============================] - 100s 315ms/step - loss: 1.2187 - accuracy: 0.5845 - val_loss: 1.2193 - val_accuracy: 0.5902
Epoch 9/40
318/318 [==============================] - 100s 315ms/step - loss: 1.1742 - accuracy: 0.5961 - val_loss: 1.2002 - val_accuracy: 0.5948
Epoch 10/40
318/318 [==============================] - 100s 314ms/step - loss: 1.1603 - accuracy: 0.6063 - val_loss: 1.1833 - val_accuracy: 0.6058
Epoch 11/40
318/318 [==============================] - 102s 320ms/step - loss: 1.1155 - accuracy: 0.6245 - val_loss: 1.2338 - val_accuracy: 0.5782
Epoch 12/40
318/318 [==============================] - 105s 330ms/step - loss: 1.1066 - accuracy: 0.6301 - val_loss: 1.1131 - val_accuracy: 0.6228
Epoch 13/40
318/318 [==============================] - 105s 329ms/step - loss: 1.0696 - accuracy: 0.6394 - val_loss: 1.1240 - val_accuracy: 0.6339
Epoch 14/40
318/318 [==============================] - 103s 322ms/step - loss: 1.0423 - accuracy: 0.6454 - val_loss: 1.0237 - val_accuracy: 0.6720
Epoch 15/40
318/318 [==============================] - 104s 328ms/step - loss: 1.0408 - accuracy: 0.6499 - val_loss: 1.1768 - val_accuracy: 0.6141
Epoch 16/40
318/318 [==============================] - 104s 326ms/step - loss: 1.0131 - accuracy: 0.6575 - val_loss: 1.1342 - val_accuracy: 0.6417
Epoch 17/40
318/318 [==============================] - 106s 332ms/step - loss: 1.0071 - accuracy: 0.6581 - val_loss: 1.1154 - val_accuracy: 0.6339
Epoch 18/40
318/318 [==============================] - 101s 318ms/step - loss: 0.9877 - accuracy: 0.6703 - val_loss: 1.0450 - val_accuracy: 0.6582
Epoch 19/40
318/318 [==============================] - 100s 313ms/step - loss: 0.9820 - accuracy: 0.6706 - val_loss: 1.0082 - val_accuracy: 0.6711
Epoch 20/40
318/318 [==============================] - 100s 315ms/step - loss: 0.9617 - accuracy: 0.6739 - val_loss: 1.0197 - val_accuracy: 0.6564
Epoch 21/40
318/318 [==============================] - 99s 312ms/step - loss: 0.9467 - accuracy: 0.6828 - val_loss: 1.1401 - val_accuracy: 0.6325
Epoch 22/40
318/318 [==============================] - 99s 312ms/step - loss: 0.9509 - accuracy: 0.6753 - val_loss: 1.0631 - val_accuracy: 0.6541
Epoch 23/40
318/318 [==============================] - 99s 312ms/step - loss: 0.9247 - accuracy: 0.6873 - val_loss: 1.0410 - val_accuracy: 0.6536
Epoch 24/40
318/318 [==============================] - 100s 313ms/step - loss: 0.9192 - accuracy: 0.6902 - val_loss: 1.0173 - val_accuracy: 0.6743
Epoch 25/40
318/318 [==============================] - 98s 309ms/step - loss: 0.8914 - accuracy: 0.6989 - val_loss: 0.9956 - val_accuracy: 0.6762
Epoch 26/40
318/318 [==============================] - 99s 310ms/step - loss: 0.8829 - accuracy: 0.6997 - val_loss: 0.9583 - val_accuracy: 0.6872
Epoch 27/40
318/318 [==============================] - 98s 309ms/step - loss: 0.8932 - accuracy: 0.6993 - val_loss: 1.0373 - val_accuracy: 0.6582
Epoch 28/40
318/318 [==============================] - 99s 311ms/step - loss: 0.8839 - accuracy: 0.7033 - val_loss: 0.9918 - val_accuracy: 0.6840
Epoch 29/40
318/318 [==============================] - 99s 312ms/step - loss: 0.8738 - accuracy: 0.6995 - val_loss: 1.0167 - val_accuracy: 0.6753
Epoch 30/40
318/318 [==============================] - 100s 315ms/step - loss: 0.8851 - accuracy: 0.6984 - val_loss: 0.9843 - val_accuracy: 0.6725
Epoch 31/40
318/318 [==============================] - 100s 313ms/step - loss: 0.8620 - accuracy: 0.7073 - val_loss: 0.9695 - val_accuracy: 0.6950
Epoch 32/40
318/318 [==============================] - 101s 319ms/step - loss: 0.8382 - accuracy: 0.7126 - val_loss: 0.9676 - val_accuracy: 0.6877
Epoch 33/40
318/318 [==============================] - 101s 316ms/step - loss: 0.8284 - accuracy: 0.7156 - val_loss: 0.9861 - val_accuracy: 0.6946
Epoch 34/40
318/318 [==============================] - 102s 319ms/step - loss: 0.8291 - accuracy: 0.7162 - val_loss: 1.0551 - val_accuracy: 0.6665
Epoch 35/40
318/318 [==============================] - 101s 317ms/step - loss: 0.8346 - accuracy: 0.7193 - val_loss: 1.0918 - val_accuracy: 0.6605
Epoch 36/40
318/318 [==============================] - 102s 320ms/step - loss: 0.8154 - accuracy: 0.7265 - val_loss: 0.9921 - val_accuracy: 0.6785
Epoch 37/40
318/318 [==============================] - 105s 331ms/step - loss: 0.8208 - accuracy: 0.7253 - val_loss: 0.9930 - val_accuracy: 0.6803
Epoch 38/40
318/318 [==============================] - 107s 336ms/step - loss: 0.7900 - accuracy: 0.7350 - val_loss: 1.0728 - val_accuracy: 0.6619
Epoch 39/40
318/318 [==============================] - 102s 321ms/step - loss: 0.8208 - accuracy: 0.7262 - val_loss: 0.9903 - val_accuracy: 0.6817
Epoch 40/40
318/318 [==============================] - 103s 322ms/step - loss: 0.7882 - accuracy: 0.7318 - val_loss: 0.9847 - val_accuracy: 0.6877
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history2_2</span> <span class="o">=</span> <span class="n">Model2_2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_ds3</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">val_ds3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/40
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.
212/212 [==============================] - 72s 324ms/step - loss: 2.5276 - accuracy: 0.2166 - val_loss: 1.8768 - val_accuracy: 0.2961
Epoch 2/40
212/212 [==============================] - 70s 331ms/step - loss: 1.7877 - accuracy: 0.3408 - val_loss: 1.7643 - val_accuracy: 0.3855
Epoch 3/40
212/212 [==============================] - 71s 332ms/step - loss: 1.6385 - accuracy: 0.4183 - val_loss: 1.6805 - val_accuracy: 0.4008
Epoch 4/40
212/212 [==============================] - 71s 335ms/step - loss: 1.5348 - accuracy: 0.4549 - val_loss: 1.4689 - val_accuracy: 0.5189
Epoch 5/40
212/212 [==============================] - 71s 336ms/step - loss: 1.4443 - accuracy: 0.4987 - val_loss: 1.4477 - val_accuracy: 0.5084
Epoch 6/40
212/212 [==============================] - 72s 338ms/step - loss: 1.3818 - accuracy: 0.5240 - val_loss: 1.3508 - val_accuracy: 0.5321
Epoch 7/40
212/212 [==============================] - 71s 336ms/step - loss: 1.3142 - accuracy: 0.5541 - val_loss: 1.3192 - val_accuracy: 0.5628
Epoch 8/40
212/212 [==============================] - 73s 345ms/step - loss: 1.2986 - accuracy: 0.5578 - val_loss: 1.2276 - val_accuracy: 0.5929
Epoch 9/40
212/212 [==============================] - 74s 349ms/step - loss: 1.2644 - accuracy: 0.5707 - val_loss: 1.2392 - val_accuracy: 0.5887
Epoch 10/40
212/212 [==============================] - 73s 344ms/step - loss: 1.2045 - accuracy: 0.5974 - val_loss: 1.2005 - val_accuracy: 0.5971
Epoch 11/40
212/212 [==============================] - 73s 345ms/step - loss: 1.1524 - accuracy: 0.6059 - val_loss: 1.2497 - val_accuracy: 0.5747
Epoch 12/40
212/212 [==============================] - 74s 348ms/step - loss: 1.1650 - accuracy: 0.6084 - val_loss: 1.1636 - val_accuracy: 0.5936
Epoch 13/40
212/212 [==============================] - 72s 339ms/step - loss: 1.1271 - accuracy: 0.6201 - val_loss: 1.1339 - val_accuracy: 0.6201
Epoch 14/40
212/212 [==============================] - 72s 339ms/step - loss: 1.1042 - accuracy: 0.6251 - val_loss: 1.1094 - val_accuracy: 0.6159
Epoch 15/40
212/212 [==============================] - 71s 336ms/step - loss: 1.0651 - accuracy: 0.6344 - val_loss: 1.1361 - val_accuracy: 0.6257
Epoch 16/40
212/212 [==============================] - 70s 331ms/step - loss: 1.0428 - accuracy: 0.6468 - val_loss: 1.0882 - val_accuracy: 0.6564
Epoch 17/40
212/212 [==============================] - 70s 330ms/step - loss: 1.0271 - accuracy: 0.6511 - val_loss: 1.1085 - val_accuracy: 0.6292
Epoch 18/40
212/212 [==============================] - 70s 331ms/step - loss: 1.0140 - accuracy: 0.6618 - val_loss: 1.0657 - val_accuracy: 0.6376
Epoch 19/40
212/212 [==============================] - 70s 329ms/step - loss: 0.9926 - accuracy: 0.6582 - val_loss: 1.0897 - val_accuracy: 0.6411
Epoch 20/40
212/212 [==============================] - 70s 330ms/step - loss: 0.9640 - accuracy: 0.6701 - val_loss: 1.1598 - val_accuracy: 0.6027
Epoch 21/40
212/212 [==============================] - 71s 332ms/step - loss: 0.9582 - accuracy: 0.6787 - val_loss: 1.0694 - val_accuracy: 0.6362
Epoch 22/40
212/212 [==============================] - 71s 336ms/step - loss: 0.9395 - accuracy: 0.6776 - val_loss: 1.0932 - val_accuracy: 0.6425
Epoch 23/40
212/212 [==============================] - 71s 335ms/step - loss: 0.9353 - accuracy: 0.6878 - val_loss: 1.0972 - val_accuracy: 0.6327
Epoch 24/40
212/212 [==============================] - 70s 330ms/step - loss: 0.9066 - accuracy: 0.6903 - val_loss: 1.0513 - val_accuracy: 0.6599
Epoch 25/40
212/212 [==============================] - 70s 330ms/step - loss: 0.8973 - accuracy: 0.6934 - val_loss: 1.0738 - val_accuracy: 0.6487
Epoch 26/40
212/212 [==============================] - 70s 331ms/step - loss: 0.9109 - accuracy: 0.6945 - val_loss: 1.0973 - val_accuracy: 0.6348
Epoch 27/40
212/212 [==============================] - 70s 331ms/step - loss: 0.9336 - accuracy: 0.6816 - val_loss: 1.0235 - val_accuracy: 0.6746
Epoch 28/40
212/212 [==============================] - 73s 342ms/step - loss: 0.8649 - accuracy: 0.6993 - val_loss: 1.0902 - val_accuracy: 0.6494
Epoch 29/40
212/212 [==============================] - 72s 340ms/step - loss: 0.8725 - accuracy: 0.7036 - val_loss: 1.0356 - val_accuracy: 0.6627
Epoch 30/40
212/212 [==============================] - 71s 337ms/step - loss: 0.8560 - accuracy: 0.7070 - val_loss: 1.0644 - val_accuracy: 0.6585
Epoch 31/40
212/212 [==============================] - 71s 334ms/step - loss: 0.8581 - accuracy: 0.7057 - val_loss: 1.1381 - val_accuracy: 0.6243
Epoch 32/40
212/212 [==============================] - 72s 342ms/step - loss: 0.8199 - accuracy: 0.7231 - val_loss: 1.1422 - val_accuracy: 0.6292
Epoch 33/40
212/212 [==============================] - 70s 331ms/step - loss: 0.8185 - accuracy: 0.7262 - val_loss: 1.1518 - val_accuracy: 0.6103
Epoch 34/40
212/212 [==============================] - 71s 334ms/step - loss: 0.7939 - accuracy: 0.7309 - val_loss: 1.0281 - val_accuracy: 0.6809
Epoch 35/40
212/212 [==============================] - 74s 348ms/step - loss: 0.8149 - accuracy: 0.7240 - val_loss: 1.1334 - val_accuracy: 0.6494
Epoch 36/40
212/212 [==============================] - 75s 354ms/step - loss: 0.7861 - accuracy: 0.7288 - val_loss: 1.1074 - val_accuracy: 0.6620
Epoch 37/40
212/212 [==============================] - 73s 345ms/step - loss: 0.7815 - accuracy: 0.7300 - val_loss: 1.1738 - val_accuracy: 0.6480
Epoch 38/40
212/212 [==============================] - 74s 346ms/step - loss: 0.7526 - accuracy: 0.7445 - val_loss: 1.0463 - val_accuracy: 0.6704
Epoch 39/40
212/212 [==============================] - 75s 353ms/step - loss: 0.7512 - accuracy: 0.7425 - val_loss: 1.1099 - val_accuracy: 0.6515
Epoch 40/40
212/212 [==============================] - 77s 361ms/step - loss: 0.7470 - accuracy: 0.7430 - val_loss: 1.0902 - val_accuracy: 0.6564
</pre></div>
</div>
</div>
</div>
<p>Das Modell mit den besten identifizierten Parametern (so wie oben konstruiert) wird nun abgespeichert:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Speichern des Models</span>
<span class="c1">#Model2_2.save(&#39;CNN2best.h5&#39;)</span>
</pre></div>
</div>
</div>
</div>
<p>Das gespeicherte Modell kann nun jederzeit wieder geladen werden. Die Evaluation erfolgt auf Basis des abgespeicherten Modells, welches mit Dataset 3 trainiert wurde.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Model2Best</span><span class="o">=</span><span class="n">load_model</span><span class="p">(</span><span class="s1">&#39;CNN2best.h5&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Mit Hilfe des folgenden Codes können wir uns die Feature Maps eines bestimmten Layers visualisieren lassen.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># definiere modell als output nach ersten hidden layer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">Model2Best</span><span class="o">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">Model2Best</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">)</span>
<span class="n">Model2Best</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
<span class="c1"># Bild mit richtiger Image Size laden</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">load_img</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;..\ProjectBirdClassification\Bilder2\chicken\0K3D5SDTO0RJ.jpg&quot;</span><span class="p">,</span> <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>
<span class="c1"># Bild zu Array konvertieren</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">img_to_array</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="c1"># Dimension expandieren</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">expand_dims</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># erzeugen der Feature Maps für ausgewählten Layer</span>
<span class="n">feature_maps</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="c1"># alle Feature Maps plotten</span>
<span class="n">square</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">ix</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">):</span>
	<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
		<span class="n">ax</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">square</span><span class="p">,</span> <span class="n">square</span><span class="p">,</span> <span class="n">ix</span><span class="p">)</span>
		<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
		<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
		<span class="n">pyplot</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">ix</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
		<span class="n">ix</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_11&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 random_flip_11 (RandomFlip)  (None, 64, 64, 3)        0         
                                                                 
 random_rotation_8 (RandomRo  (None, 64, 64, 3)        0         
 tation)                                                         
                                                                 
 conv2d_53 (Conv2D)          (None, 62, 62, 32)        896       
                                                                 
 conv2d_54 (Conv2D)          (None, 60, 60, 32)        9248      
                                                                 
 max_pooling2d_30 (MaxPoolin  (None, 30, 30, 32)       0         
 g2D)                                                            
                                                                 
 dropout_8 (Dropout)         (None, 30, 30, 32)        0         
                                                                 
 conv2d_55 (Conv2D)          (None, 28, 28, 64)        18496     
                                                                 
 conv2d_56 (Conv2D)          (None, 26, 26, 64)        36928     
                                                                 
 max_pooling2d_31 (MaxPoolin  (None, 13, 13, 64)       0         
 g2D)                                                            
                                                                 
 conv2d_57 (Conv2D)          (None, 11, 11, 128)       73856     
                                                                 
 conv2d_58 (Conv2D)          (None, 9, 9, 128)         147584    
                                                                 
 max_pooling2d_32 (MaxPoolin  (None, 4, 4, 128)        0         
 g2D)                                                            
                                                                 
 flatten_11 (Flatten)        (None, 2048)              0         
                                                                 
 dense_32 (Dense)            (None, 256)               524544    
                                                                 
 dense_33 (Dense)            (None, 256)               65792     
                                                                 
 dropout_9 (Dropout)         (None, 256)               0         
                                                                 
 dense_34 (Dense)            (None, 9)                 2313      
                                                                 
=================================================================
Total params: 879,657
Trainable params: 879,657
Non-trainable params: 0
_________________________________________________________________
WARNING:tensorflow:6 out of the last 54 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x000002CA9B9C9790&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
1/1 [==============================] - 0s 33ms/step
</pre></div>
</div>
<img alt="../_images/ModellingFinal_140_1.png" src="../_images/ModellingFinal_140_1.png" />
</div>
</div>
<p>An den geplotetten Feature Maps können wir bereits erkennen, welche Merkmale das Modell in dem angegebenen Layer (in diesem Fall Layer 2, erster convolutional Layer) extrahiert.<br />
Zum Vergleich printen wir hier das normale Bild:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">load_img</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;..\ProjectBirdClassification\Bilder2\chicken\0K3D5SDTO0RJ.jpg&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_142_0.png" src="../_images/ModellingFinal_142_0.png" />
</div>
</div>
</section>
<section id="d-model-2-evaluation">
<h3>d) Model 2 Evaluation<a class="headerlink" href="#d-model-2-evaluation" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy und Loss des komplexeren CNN mit dataset 2:&quot;</span><span class="p">)</span>
<span class="n">plot_accuracy</span><span class="p">(</span><span class="n">history2_1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy und Loss des komplexeren CNN mit dataset 3:&quot;</span><span class="p">)</span>
<span class="n">plot_accuracy</span><span class="p">(</span><span class="n">history2_2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy und Loss des komplexeren CNN mit dataset 2:
Maximum accuracy:  0.6950321793556213
Minimum Loss:  0.9582885503768921
</pre></div>
</div>
<img alt="../_images/ModellingFinal_144_1.png" src="../_images/ModellingFinal_144_1.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy und Loss des komplexeren CNN mit dataset 3:
Maximum accuracy:  0.6808659434318542
Minimum Loss:  1.023476481437683
</pre></div>
</div>
<img alt="../_images/ModellingFinal_144_3.png" src="../_images/ModellingFinal_144_3.png" />
</div>
</div>
<p>Hier fällt auf, dass beim Training teilweise stärkere Schwankungen in der Validation Accuracy auftreten. Dies liegt vermutlich daran, dass das Modell noch relativ häufig am raten ist.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model2_corL</span><span class="p">,</span> <span class="n">model2_predL</span> <span class="o">=</span> <span class="n">predict_testdata</span><span class="p">(</span><span class="n">Model2Best</span><span class="p">,</span> <span class="n">test_ds3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1/1 [==============================] - 0s 40ms/step
1/1 [==============================] - 0s 38ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 38ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 38ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 40ms/step
1/1 [==============================] - 0s 40ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 40ms/step
1/1 [==============================] - 0s 38ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 38ms/step
1/1 [==============================] - 0s 38ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 38ms/step
1/1 [==============================] - 0s 40ms/step
1/1 [==============================] - 0s 38ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 41ms/step
1/1 [==============================] - 0s 40ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 42ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 38ms/step
1/1 [==============================] - 0s 41ms/step
1/1 [==============================] - 0s 42ms/step
1/1 [==============================] - 0s 41ms/step
1/1 [==============================] - 0s 42ms/step
1/1 [==============================] - 0s 40ms/step
1/1 [==============================] - 0s 40ms/step
1/1 [==============================] - 0s 39ms/step
1/1 [==============================] - 0s 40ms/step
1/1 [==============================] - 0s 44ms/step
1/1 [==============================] - 0s 40ms/step
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">model2_corL</span><span class="p">,</span> <span class="n">model2_predL</span><span class="p">,</span><span class="n">target_names</span> <span class="o">=</span> <span class="n">class_names</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>              precision    recall  f1-score   support

     chicken       0.55      0.85      0.67       182
    cockatoo       0.93      0.28      0.43       132
       eagle       0.81      0.61      0.70       130
    flamingo       0.91      0.67      0.77       155
     ostrich       0.47      0.93      0.62       164
         owl       0.59      0.46      0.52       121
     penguin       0.74      0.63      0.68       191
         tit       0.88      0.73      0.80       202
       tucan       0.87      0.85      0.86       195

    accuracy                           0.69      1472
   macro avg       0.75      0.67      0.67      1472
weighted avg       0.75      0.69      0.69      1472
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">model2_corL</span><span class="p">,</span> <span class="n">model2_predL</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_148_0.png" src="../_images/ModellingFinal_148_0.png" />
</div>
</div>
<p>die zwei Klassen mit dem schlechsten F1-Score sind hier cockatoo und owl:
Bei der Klasse cockatoo fällt auf, dass die Precision sehr hoch ist (fast alle der als cockatoo klassifizierten Bilder sind tatsächlich cockatoos), allerdings der Recall sehr schlecht ist (nur wenige der als cockatoo wahr klassifierten Testbilder wurden als solche erkannt).</p>
<p>Bei der Klasse owl dagegen sind sowohl Precision als auch Recall schlecht. Wie bereits in Kapitel 4 vermutet, könnte dies an der relativ großen Intraclass Varianz liegen, welche zuvor erörtert wurde.</p>
<p>Zum Abschluss wählen wir noch einmal komplett neue Bilder aus, um diese vom Modell klassifizieren zu lassen. Dazu wird die Funktion <em>predict_image</em> erstellt und auf die Bilder angewendet.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">predict_image</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">image_size</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>

    <span class="c1"># Laden des Bilds in der angegebenen Größe</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">load_img</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">target_size</span><span class="o">=</span><span class="n">image_size</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img_to_array</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint8&quot;</span><span class="p">))</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">expand_dims</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Klassifikation durchführen</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

    <span class="n">dic</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">class_names</span><span class="p">,</span> <span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

    <span class="c1">#Sortieren der Vorhersagen nach Wahrscheinlichkeit</span>
    <span class="n">sorted_d</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">dic</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="n">operator</span><span class="o">.</span><span class="n">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span><span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sorted_d</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predict_image</span><span class="p">(</span><span class="s2">&quot;..\ProjectBirdClassification\Testbilder\madagascar-penguin.jpg&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span> <span class="n">Model2Best</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1/1 [==============================] - 0s 22ms/step
{&#39;penguin&#39;: 0.9894449, &#39;tucan&#39;: 0.0071074343, &#39;chicken&#39;: 0.001482426, &#39;owl&#39;: 0.0010628579, &#39;eagle&#39;: 0.0008068173, &#39;tit&#39;: 8.9578425e-05, &#39;flamingo&#39;: 3.0716665e-06, &#39;cockatoo&#39;: 2.3442938e-06, &#39;ostrich&#39;: 5.248481e-07}
</pre></div>
</div>
<img alt="../_images/ModellingFinal_152_1.png" src="../_images/ModellingFinal_152_1.png" />
</div>
</div>
<p><strong>Originalbild:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">load_img</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;..\ProjectBirdClassification\Testbilder\madagascar-penguin.jpg&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_154_0.png" src="../_images/ModellingFinal_154_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predict_image</span><span class="p">(</span><span class="s2">&quot;..\ProjectBirdClassification\Testbilder\TucanHaribo.jpg&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span> <span class="n">Model2Best</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1/1 [==============================] - 0s 21ms/step
{&#39;tucan&#39;: 0.8887226, &#39;chicken&#39;: 0.092569664, &#39;eagle&#39;: 0.014680784, &#39;ostrich&#39;: 0.0020942164, &#39;owl&#39;: 0.00092992565, &#39;flamingo&#39;: 0.00055281224, &#39;penguin&#39;: 0.00027359833, &#39;tit&#39;: 0.00017552107, &#39;cockatoo&#39;: 8.70308e-07}
</pre></div>
</div>
<img alt="../_images/ModellingFinal_155_1.png" src="../_images/ModellingFinal_155_1.png" />
</div>
</div>
<p><strong>Originalbild:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">load_img</span><span class="p">(</span><span class="s2">&quot;..\ProjectBirdClassification\Testbilder\TucanHaribo.jpg&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_157_0.png" src="../_images/ModellingFinal_157_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predict_image</span><span class="p">(</span><span class="s2">&quot;..\ProjectBirdClassification\Testbilder\Hedwig.jpg&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span> <span class="n">Model2Best</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1/1 [==============================] - 0s 22ms/step
{&#39;tucan&#39;: 0.9107473, &#39;eagle&#39;: 0.045005497, &#39;flamingo&#39;: 0.02605853, &#39;penguin&#39;: 0.007454081, &#39;owl&#39;: 0.006661349, &#39;chicken&#39;: 0.0023901935, &#39;ostrich&#39;: 0.001147009, &#39;tit&#39;: 0.00045412677, &#39;cockatoo&#39;: 8.196658e-05}
</pre></div>
</div>
<img alt="../_images/ModellingFinal_158_1.png" src="../_images/ModellingFinal_158_1.png" />
</div>
</div>
<p><strong>Originalbild:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">load_img</span><span class="p">(</span><span class="s2">&quot;..\ProjectBirdClassification\Testbilder\Hedwig.jpg&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_160_0.png" src="../_images/ModellingFinal_160_0.png" />
</div>
</div>
<p>Der Pinguin und der Tucan konnten vom Modell korrekt erkannt werden, die Eule wurde allerdings evtl. auf Grund des bunten Schals als Tucan klassifiziert.</p>
</section>
<section id="e-model-2-fazit">
<h3>e) Model 2 - Fazit<a class="headerlink" href="#e-model-2-fazit" title="Permalink to this headline">#</a></h3>
<p>Wie erwartet performt Modell 2 auf Grund der komplexeren Architektur und den vielen erprobten Parameteroptionen und Kombinationen besser als Modell 1. Das Ziel von 80% Accuracy konnte allerdings nicht erreicht werden. Dies liegt vermutlich an einer Kombinationen von nicht perfekten Trainingsdaten (Zu wenige Bilder, teilweise hohe Varianz in Klassen) und beschränkter Ressourcen zum Trainieren auf dem lokalen Rechner.</p>
<p>Um den letzten Punkt zu prüfen, soll noch ein weiteres Modell mit noch komplexerer Architektur erstellt werden, um zu testen ob dies die Performance weiter verbessern kann.</p>
</section>
</section>
<section id="model-3">
<h2>6.3 Model 3<a class="headerlink" href="#model-3" title="Permalink to this headline">#</a></h2>
<section id="c-model-3-introduction">
<h3>c) Model 3 - Introduction<a class="headerlink" href="#c-model-3-introduction" title="Permalink to this headline">#</a></h3>
<p>Als letztes selbst erstelltes Modell soll nun mit dem zuvor erworbenen Wissen ein CNN mit noch mehr Layern und Filtern erstellt werden um zu überprüfen, ob dies die Performance weiterhin optimieren kann. Dazu wird die in Modell 2 bewährte Architektur um einen zusätzlichen convolutional Layer ergänzt.</p>
</section>
<section id="a-model-3-construction">
<h3>a) Model 3 - Construction<a class="headerlink" href="#a-model-3-construction" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">construct_cnn3</span><span class="p">(</span><span class="n">learningRate</span><span class="p">):</span>
    <span class="n">cnn3</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    <span class="n">cnn3</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">RandomFlip</span><span class="p">(</span><span class="s2">&quot;horizontal_and_vertical&quot;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span><span class="mi">3</span><span class="p">)))</span>
    <span class="n">cnn3</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">RandomRotation</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
    <span class="n">cnn3</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">cnn3</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">cnn3</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPool2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
    <span class="n">cnn3</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
    <span class="n">cnn3</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">cnn3</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">cnn3</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPool2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
    <span class="n">cnn3</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">cnn3</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">cnn3</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPool2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
    <span class="n">cnn3</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">cnn3</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
    <span class="n">cnn3</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">cnn3</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
    <span class="n">cnn3</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">learningRate</span><span class="p">)</span>
    <span class="n">cnn3</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">cnn3</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Architektur und Parameter:</strong><br />
Für dieses Modell werden die zuvor als optimal getesteten Parameter gewählt und lediglich mit dem hinzufügen von zusätzlichen Layern experimentiert.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Model3</span><span class="o">=</span><span class="n">construct_cnn3</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Model3</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;Adam&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visuelle Darstellung der Layer von Modell 3</span>
<span class="n">visualkeras</span><span class="o">.</span><span class="n">layered_view</span><span class="p">(</span><span class="n">Model3</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_172_0.png" src="../_images/ModellingFinal_172_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Model3</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_46&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 random_flip_34 (RandomFlip)  (None, 64, 64, 3)        0         
                                                                 
 random_rotation_31 (RandomR  (None, 64, 64, 3)        0         
 otation)                                                        
                                                                 
 conv2d_235 (Conv2D)         (None, 62, 62, 64)        1792      
                                                                 
 conv2d_236 (Conv2D)         (None, 60, 60, 64)        36928     
                                                                 
 max_pooling2d_108 (MaxPooli  (None, 30, 30, 64)       0         
 ng2D)                                                           
                                                                 
 dropout_75 (Dropout)        (None, 30, 30, 64)        0         
                                                                 
 conv2d_237 (Conv2D)         (None, 28, 28, 128)       73856     
                                                                 
 conv2d_238 (Conv2D)         (None, 26, 26, 128)       147584    
                                                                 
 max_pooling2d_109 (MaxPooli  (None, 13, 13, 128)      0         
 ng2D)                                                           
                                                                 
 conv2d_239 (Conv2D)         (None, 11, 11, 256)       295168    
                                                                 
 conv2d_240 (Conv2D)         (None, 9, 9, 256)         590080    
                                                                 
 max_pooling2d_110 (MaxPooli  (None, 4, 4, 256)        0         
 ng2D)                                                           
                                                                 
 conv2d_241 (Conv2D)         (None, 2, 2, 1024)        2360320   
                                                                 
 flatten_38 (Flatten)        (None, 4096)              0         
                                                                 
 dense_97 (Dense)            (None, 512)               2097664   
                                                                 
 dropout_76 (Dropout)        (None, 512)               0         
                                                                 
 dense_98 (Dense)            (None, 9)                 4617      
                                                                 
=================================================================
Total params: 5,608,009
Trainable params: 5,608,009
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
</section>
<section id="b-model-3-training">
<h3>b) Model 3 - Training<a class="headerlink" href="#b-model-3-training" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history3</span> <span class="o">=</span> <span class="n">Model3</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_ds3</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">val_ds3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/20
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.
212/212 [==============================] - 179s 833ms/step - loss: 2.3881 - accuracy: 0.2088 - val_loss: 2.0290 - val_accuracy: 0.1878
Epoch 2/20
212/212 [==============================] - 178s 838ms/step - loss: 1.8918 - accuracy: 0.2849 - val_loss: 1.7160 - val_accuracy: 0.3624
Epoch 3/20
212/212 [==============================] - 178s 840ms/step - loss: 1.7642 - accuracy: 0.3463 - val_loss: 1.7519 - val_accuracy: 0.3541
Epoch 4/20
212/212 [==============================] - 179s 843ms/step - loss: 1.6733 - accuracy: 0.3944 - val_loss: 1.7071 - val_accuracy: 0.3722
Epoch 5/20
212/212 [==============================] - 181s 855ms/step - loss: 1.5811 - accuracy: 0.4432 - val_loss: 1.5954 - val_accuracy: 0.4392
Epoch 6/20
212/212 [==============================] - 181s 853ms/step - loss: 1.5206 - accuracy: 0.4677 - val_loss: 1.4064 - val_accuracy: 0.5119
Epoch 7/20
212/212 [==============================] - 180s 847ms/step - loss: 1.4515 - accuracy: 0.4950 - val_loss: 1.5034 - val_accuracy: 0.4749
Epoch 8/20
212/212 [==============================] - 180s 848ms/step - loss: 1.4164 - accuracy: 0.5043 - val_loss: 1.3194 - val_accuracy: 0.5377
Epoch 9/20
212/212 [==============================] - 180s 848ms/step - loss: 1.3774 - accuracy: 0.5212 - val_loss: 1.3711 - val_accuracy: 0.5133
Epoch 10/20
212/212 [==============================] - 179s 846ms/step - loss: 1.3572 - accuracy: 0.5335 - val_loss: 1.2896 - val_accuracy: 0.5642
Epoch 11/20
212/212 [==============================] - 181s 855ms/step - loss: 1.3077 - accuracy: 0.5462 - val_loss: 1.2752 - val_accuracy: 0.5538
Epoch 12/20
212/212 [==============================] - 175s 824ms/step - loss: 1.2734 - accuracy: 0.5553 - val_loss: 1.2423 - val_accuracy: 0.5712
Epoch 13/20
212/212 [==============================] - 160s 755ms/step - loss: 1.2525 - accuracy: 0.5692 - val_loss: 1.2221 - val_accuracy: 0.5803
Epoch 14/20
212/212 [==============================] - 160s 754ms/step - loss: 1.2394 - accuracy: 0.5730 - val_loss: 1.2165 - val_accuracy: 0.6075
Epoch 15/20
212/212 [==============================] - 159s 749ms/step - loss: 1.1955 - accuracy: 0.5949 - val_loss: 1.2436 - val_accuracy: 0.5859
Epoch 16/20
212/212 [==============================] - 159s 751ms/step - loss: 1.1894 - accuracy: 0.5962 - val_loss: 1.2366 - val_accuracy: 0.5810
Epoch 17/20
212/212 [==============================] - 160s 753ms/step - loss: 1.1625 - accuracy: 0.6015 - val_loss: 1.2030 - val_accuracy: 0.5943
Epoch 18/20
212/212 [==============================] - 159s 749ms/step - loss: 1.1307 - accuracy: 0.6077 - val_loss: 1.1455 - val_accuracy: 0.6236
Epoch 19/20
212/212 [==============================] - 159s 748ms/step - loss: 1.1141 - accuracy: 0.6160 - val_loss: 1.1866 - val_accuracy: 0.6208
Epoch 20/20
212/212 [==============================] - 159s 748ms/step - loss: 1.0938 - accuracy: 0.6350 - val_loss: 1.2148 - val_accuracy: 0.5880
</pre></div>
</div>
</div>
</div>
</section>
<section id="a-model-3-evaluation">
<h3>a) Model 3 - Evaluation<a class="headerlink" href="#a-model-3-evaluation" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy und Loss des komplexen CNN mit dataset 3:&quot;</span><span class="p">)</span>
<span class="n">plot_accuracy</span><span class="p">(</span><span class="n">history3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy und Loss des komplexen CNN mit dataset 3:
Maximum accuracy:  0.623603343963623
Minimum Loss:  1.145463466644287
</pre></div>
</div>
<img alt="../_images/ModellingFinal_177_1.png" src="../_images/ModellingFinal_177_1.png" />
</div>
</div>
<p>Nach einigen Tests mit weiteren Layern konnte keine wesentliche Verbesserung zu Modell 2 festgestellt werden. Modell 3 zeigt ähnliche Ergebnisse und Schwächen auf wie Modell 3, trotz zusätzlicher Layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model3_corL</span><span class="p">,</span> <span class="n">model3_predL</span> <span class="o">=</span> <span class="n">predict_testdata</span><span class="p">(</span><span class="n">Model3</span><span class="p">,</span> <span class="n">test_ds3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1/1 [==============================] - 0s 340ms/step
1/1 [==============================] - 0s 204ms/step
1/1 [==============================] - 0s 196ms/step
1/1 [==============================] - 0s 195ms/step
1/1 [==============================] - 0s 194ms/step
1/1 [==============================] - 0s 196ms/step
1/1 [==============================] - 0s 194ms/step
1/1 [==============================] - 0s 196ms/step
1/1 [==============================] - 0s 193ms/step
1/1 [==============================] - 0s 195ms/step
1/1 [==============================] - 0s 195ms/step
1/1 [==============================] - 0s 195ms/step
1/1 [==============================] - 0s 195ms/step
1/1 [==============================] - 0s 202ms/step
1/1 [==============================] - 0s 193ms/step
1/1 [==============================] - 0s 191ms/step
1/1 [==============================] - 0s 189ms/step
1/1 [==============================] - 0s 193ms/step
1/1 [==============================] - 0s 190ms/step
1/1 [==============================] - 0s 195ms/step
1/1 [==============================] - 0s 197ms/step
1/1 [==============================] - 0s 196ms/step
1/1 [==============================] - 0s 189ms/step
1/1 [==============================] - 0s 192ms/step
1/1 [==============================] - 0s 192ms/step
1/1 [==============================] - 0s 192ms/step
1/1 [==============================] - 0s 186ms/step
1/1 [==============================] - 0s 185ms/step
1/1 [==============================] - 0s 185ms/step
1/1 [==============================] - 0s 185ms/step
1/1 [==============================] - 0s 184ms/step
1/1 [==============================] - 0s 182ms/step
1/1 [==============================] - 0s 180ms/step
1/1 [==============================] - 0s 181ms/step
1/1 [==============================] - 0s 179ms/step
1/1 [==============================] - 0s 178ms/step
1/1 [==============================] - 0s 172ms/step
1/1 [==============================] - 0s 171ms/step
1/1 [==============================] - 0s 171ms/step
1/1 [==============================] - 0s 172ms/step
1/1 [==============================] - 0s 166ms/step
1/1 [==============================] - 0s 164ms/step
1/1 [==============================] - 0s 166ms/step
1/1 [==============================] - 0s 169ms/step
1/1 [==============================] - 0s 167ms/step
1/1 [==============================] - 0s 172ms/step
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">model3_corL</span><span class="p">,</span> <span class="n">model3_predL</span><span class="p">,</span><span class="n">target_names</span> <span class="o">=</span> <span class="n">class_names</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>              precision    recall  f1-score   support

     chicken       0.69      0.61      0.65       192
    cockatoo       0.49      0.53      0.51       139
       eagle       0.58      0.53      0.56       120
    flamingo       0.80      0.65      0.72       159
     ostrich       0.53      0.43      0.47       162
         owl       0.37      0.61      0.46       120
     penguin       0.54      0.43      0.48       184
         tit       0.76      0.73      0.75       200
       tucan       0.65      0.82      0.73       196

    accuracy                           0.60      1472
   macro avg       0.60      0.59      0.59      1472
weighted avg       0.62      0.60      0.60      1472
</pre></div>
</div>
</div>
</div>
<p>Wie bei Modell 2 wurde die Klasse “owl” wieder am Schlechtesten erkannt, aus vermutlich selben Gründen.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">model3_corL</span><span class="p">,</span> <span class="n">model3_predL</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_182_0.png" src="../_images/ModellingFinal_182_0.png" />
</div>
</div>
</section>
<section id="e-model-3-fazit">
<h3>e) Model 3 - Fazit<a class="headerlink" href="#e-model-3-fazit" title="Permalink to this headline">#</a></h3>
<p>Leider konnte in Modell 3 die Performance von Modell 2 nicht weiter optimiert werden. Auf Grund der längeren Trainingsdauer von noch komplexeren Modellen wird dies daher an dieser Stelle als Grund genommen, um nicht mehr weiter mit noch komplexeren Architekturen zu experimentieren.</p>
<p>Um jedoch ein Gefühl für mögliche bessere Ergebnisse zu bekommen, sollen im nächsten Schritt vortrainierte Modelle angewendet und auf diesen Anwendungsfall optimiert werden.</p>
</section>
</section>
<section id="model-4-pretrained-vgg16-neural-network">
<h2>6.4 Model 4 - Pretrained VGG16 Neural Network<a class="headerlink" href="#model-4-pretrained-vgg16-neural-network" title="Permalink to this headline">#</a></h2>
<section id="a-vgg16-introduction">
<h3>a) VGG16 - Introduction<a class="headerlink" href="#a-vgg16-introduction" title="Permalink to this headline">#</a></h3>
<p>Das Very Deep Convolutional Networks for Large-Scale Image Recognition(VGG-16) CNN ist eines der bekanntesten vortrainierten Neuronalen Netze zur Bildklassifizierung. Es wurde mit Millionen von Bildern des ImageNet Datensetz trainiert und ist in der Lage zwischen 1000 verschiedenen Output Labels zu unterscheiden.<br />
Nachfolgend ist die Architektur des Modell abgebildet. Diese setzt sich aus 13 Convolutional, 5 MaxPooling sowie 3 DenseLayern zusammen.</p>
<p><img alt="Architektur VGG16 Modell" src="_sources/VGG16-architecture.PNG" /></p>
<p>(Bildquelle1: <a class="reference external" href="https://www.learndatasci.com/tutorials/hands-on-transfer-learning-keras/">https://www.learndatasci.com/tutorials/hands-on-transfer-learning-keras/</a>)</p>
<p>Um das Modell auf einen eigenen Datensatz anwenden zu können, werden lediglich die Convolutional und MaxPooling Layer mit ihren Gewichten verwendet. An deren Output wird eine eigene Architektur aus fully-connected Layern gehängt. Wichtig ist hierbei, dass die Anzahl der Neuronen im letzten Dense Layer der Anzahl der Klassen des jeweiligen Use Cases entspricht, in unserem Projekt also 9 Neuronen für 9 verschiedene Vogelarten.</p>
</section>
<section id="b-vgg16-construction">
<h3>b) VGG16 - Construction<a class="headerlink" href="#b-vgg16-construction" title="Permalink to this headline">#</a></h3>
<p>In diesem Teil wird ein Neuronales Netz erstellt, welches die Convolutional und MaxPooling Layer des VGG16 Modells verwendet. An diese wird eine eigene Zusammensetzung aus Flatten, Dense und Dropout Layern gehängt.</p>
<p>Im Rahmen der Modelloptimierung wurden verschiedene Kombinationen aus Dense Layern, Dropout und Pooling Layern, sowie verschiedene Werte der Hyperparameter Image_Size, Optimizer, Learning Rate, usw. getestet. Nachfolgend wird das Modell dargestellt, welches in dieser Testphase das beste Ergebnis erzielte.<br />
Für dieses Modell wurde das Dataset 2 erneut eingelesen mit image_size 128x128.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Falls ein trainiertes Modell schon existiert, kann dies hier geladen werden. Momentan auskommentiert</span>
<span class="n">VGG16</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s1">&#39;VGG16-128px-64dense.h5&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Laden des Basis Modells mit der VGG16 Architektur</span>
<span class="n">baseModelVGG16</span> <span class="o">=</span> <span class="n">vgg16</span><span class="o">.</span><span class="n">VGG16</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s2">&quot;imagenet&quot;</span><span class="p">,</span> <span class="n">include_top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">input_tensor</span><span class="o">=</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Visualisierung des VGG16 Basis Modells</span>
<span class="n">visualkeras</span><span class="o">.</span><span class="n">layered_view</span><span class="p">(</span><span class="n">baseModelVGG16</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_192_0.png" src="../_images/ModellingFinal_192_0.png" />
</div>
</div>
<p>Wie bereits in der Einleitung dieses Kapitels gezeigt, ist in der obigen Abbildung zu erkennen, dass sich das VGG16 Basis Model aus 13 Convolutional Layern und 5 MaxPooling Layern zusammensetzt.<br />
Die Gewichte dieser Layer wurden zusammen mit der Architektur heruntergeladen. Diese werden nun auf trainable = False gestellt, sodass sich die Gewichte in der Trainingsphase des Modells nicht mehr verändern. In dieser sollen lediglich die Gewichte der selbst hinzugefügten Layer verändert werden.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Gewichte in den Layern des Basis Modells einfrieren, sodass diese in der Trainingsphase nicht geupdatet werden</span>
<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">baseModelVGG16</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
    <span class="n">layer</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
<p>Für das Preprocessing der Bilder bietet Keras VGG16 eine eigene Methode preprocess_input. Allerdings wurden unter Verwendung dieser Methode für train_ds und val_ds deutlich schlechtere Ergebnisse des Modells erzielt. Daher wird die Methode in diesem Notebook nicht verwendet, sondern ein eigenes Preprocessing der Bilder durchgeführt.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Preprocessing Methode von VGG16, die aber nicht verwendet wird</span>
<span class="c1">#for batch_images, batch_labels in train_ds:</span>
<span class="c1">#    batch_images = vgg16.preprocess_input(batch_images, mode=&#39;tf&#39;)</span>
</pre></div>
</div>
</div>
</div>
<p>Nun wird die Architektur des gesamten Modells bestimmt. Diesem wird zunächst ein Rescaling Layer hinzugefügt, welches das Intervall der einzelnen Bildpixel von [0,255] auf [0,1] reduziert. Anschließend wird das Basis Modell angehängt. Es folgen ein MaxPooling, Flatten, Dense und Dropout Layer bevor der finale Dense Layer mit 9 Output Neuronen angehängt wird.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Architektur des kompletten Modells</span>
<span class="n">VGG16</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">VGG16</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Rescaling</span><span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="mi">255</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span><span class="mi">3</span><span class="p">)))</span>
<span class="n">VGG16</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">baseModelVGG16</span><span class="p">)</span>
<span class="n">VGG16</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPool2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="n">VGG16</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">VGG16</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>
<span class="n">VGG16</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="n">VGG16</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">))</span>
<span class="n">VGG16</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_48&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 rescaling_4 (Rescaling)     (None, 128, 128, 3)       0         
                                                                 
 vgg16 (Functional)          (None, 4, 4, 512)         14714688  
                                                                 
 max_pooling2d_112 (MaxPooli  (None, 2, 2, 512)        0         
 ng2D)                                                           
                                                                 
 flatten_40 (Flatten)        (None, 2048)              0         
                                                                 
 dense_102 (Dense)           (None, 64)                131136    
                                                                 
 dropout_79 (Dropout)        (None, 64)                0         
                                                                 
 dense_103 (Dense)           (None, 9)                 585       
                                                                 
=================================================================
Total params: 14,846,409
Trainable params: 131,721
Non-trainable params: 14,714,688
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<p>Nachfolgend ist die Architektur des gesamten Modells dargestellt, wobei das Basis Modell nur wie ein einzelner Layer erscheint. Dieser Layer beinhaltet die oben gezeigte Architektur.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">visualkeras</span><span class="o">.</span><span class="n">layered_view</span><span class="p">(</span><span class="n">VGG16</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_200_0.png" src="../_images/ModellingFinal_200_0.png" />
</div>
</div>
</section>
<section id="c-vgg16-training">
<h3>c) VGG16 - Training<a class="headerlink" href="#c-vgg16-training" title="Permalink to this headline">#</a></h3>
<p>Nun folgt das Training des Modells. Dafür wird es zunächst kompiliert und anschließend unter Verwendung der .fit() Methode über 10 Epochen trainiert. Wichtig ist hierbei nochmal, dass lediglich die Gewichte der fully-connected layer trainiert werden. Die Gewichte des VGG16 BaseModels werden nicht verändert.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Compiling VGG16 Model&quot;</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">VGG16</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Compiling VGG16 Model
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fitten des vortrainierten VGG16 Netzes mit dataset 2 (in Notebook 1 erzeugte Bilder + Bilder von https://images.cv/)&quot;</span><span class="p">)</span>
<span class="n">history6</span> <span class="o">=</span> <span class="n">VGG16</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_ds2</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size2</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">val_ds2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fitten des vortrainierten VGG16 Netzes mit dataset 2 (in Notebook 1 erzeugte Bilder + Bilder von https://images.cv/)
Epoch 1/10
318/318 [==============================] - 656s 2s/step - loss: 1.9229 - accuracy: 0.2923 - val_loss: 1.4134 - val_accuracy: 0.5782
Epoch 2/10
318/318 [==============================] - 620s 2s/step - loss: 1.5027 - accuracy: 0.4508 - val_loss: 1.1370 - val_accuracy: 0.6573
Epoch 3/10
318/318 [==============================] - 619s 2s/step - loss: 1.3853 - accuracy: 0.4991 - val_loss: 0.9714 - val_accuracy: 0.7075
Epoch 4/10
318/318 [==============================] - 624s 2s/step - loss: 1.2974 - accuracy: 0.5282 - val_loss: 1.0284 - val_accuracy: 0.6766
Epoch 5/10
318/318 [==============================] - 621s 2s/step - loss: 1.2705 - accuracy: 0.5400 - val_loss: 0.9639 - val_accuracy: 0.7236
Epoch 6/10
318/318 [==============================] - 619s 2s/step - loss: 1.2352 - accuracy: 0.5426 - val_loss: 0.8873 - val_accuracy: 0.7300
Epoch 7/10
318/318 [==============================] - 618s 2s/step - loss: 1.1811 - accuracy: 0.5633 - val_loss: 0.9073 - val_accuracy: 0.6960
Epoch 8/10
318/318 [==============================] - 613s 2s/step - loss: 1.1522 - accuracy: 0.5743 - val_loss: 0.8962 - val_accuracy: 0.7259
Epoch 9/10
318/318 [==============================] - 566s 2s/step - loss: 1.1629 - accuracy: 0.5698 - val_loss: 0.8532 - val_accuracy: 0.7401
Epoch 10/10
318/318 [==============================] - 564s 2s/step - loss: 1.1286 - accuracy: 0.5890 - val_loss: 0.8397 - val_accuracy: 0.7401
</pre></div>
</div>
</div>
</div>
<p>Nach 10 Epochen hat das Modell eine Validation Accuracy von ca. 74%. Dies ist zwar noch ausbaufähig, übertrifft aber alle bisherigen Ergebnisse mit den selbst erstellten CNNs.<br />
Auffällig ist, dass die Training Accuracy schlechter ausfällt als die Validation Accuracy. Die Erklärung hierführ könnte in dem Dropout Layer liegen, welcher zwischen die beiden Dense Layer in den fully-connected layern geschaltet wurde. Dieser deaktiviert in der Trainingsphase 50% der Neuronen, was die Accuracy in der Trainingsphase natürlich verschlechtert. Dies soll ein Overfitting des Modells verhindern. In der Validierungsphase am Ende einer Epoche werden alle Neuronen des Neuronalen Netzes aktiviert. Somit fällt die Accuracy besser aus als in der Trainingsphase.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Speichern des Modells</span>
<span class="n">VGG16</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;VGG16-128px-64dense.h5&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Nachfolgend betrachten wir die 64 Feature Maps des ersten Hidden Layer im VGG116 Modell.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># definiere modell als output nach ersten hidden layer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">baseModelVGG16</span><span class="o">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">baseModelVGG16</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">)</span>
<span class="n">baseModelVGG16</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
<span class="c1"># Bild mit richtiger Image Size laden</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">load_img</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;..\ProjectBirdClassification\Bilder2\chicken\0K3D5SDTO0RJ.jpg&quot;</span><span class="p">,</span> <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="c1"># Bild zu Array konvertieren</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">img_to_array</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="c1"># Dimension expandieren</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">expand_dims</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># Peprocess Bild für VGG16</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">vgg16</span><span class="o">.</span><span class="n">preprocess_input</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="c1"># erzeugen der Feature Maps für ausgewählten Layer</span>
<span class="n">feature_maps</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="c1"># alle Feature Maps plotten</span>
<span class="n">square</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">ix</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">square</span><span class="p">):</span>
	<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">square</span><span class="p">):</span>
		<span class="n">ax</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">square</span><span class="p">,</span> <span class="n">square</span><span class="p">,</span> <span class="n">ix</span><span class="p">)</span>
		<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
		<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
		<span class="n">pyplot</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">ix</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
		<span class="n">ix</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;vgg16&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 128, 128, 3)]     0         
                                                                 
 block1_conv1 (Conv2D)       (None, 128, 128, 64)      1792      
                                                                 
 block1_conv2 (Conv2D)       (None, 128, 128, 64)      36928     
                                                                 
 block1_pool (MaxPooling2D)  (None, 64, 64, 64)        0         
                                                                 
 block2_conv1 (Conv2D)       (None, 64, 64, 128)       73856     
                                                                 
 block2_conv2 (Conv2D)       (None, 64, 64, 128)       147584    
                                                                 
 block2_pool (MaxPooling2D)  (None, 32, 32, 128)       0         
                                                                 
 block3_conv1 (Conv2D)       (None, 32, 32, 256)       295168    
                                                                 
 block3_conv2 (Conv2D)       (None, 32, 32, 256)       590080    
                                                                 
 block3_conv3 (Conv2D)       (None, 32, 32, 256)       590080    
                                                                 
 block3_pool (MaxPooling2D)  (None, 16, 16, 256)       0         
                                                                 
 block4_conv1 (Conv2D)       (None, 16, 16, 512)       1180160   
                                                                 
 block4_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   
                                                                 
 block4_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   
                                                                 
 block4_pool (MaxPooling2D)  (None, 8, 8, 512)         0         
                                                                 
 block5_conv1 (Conv2D)       (None, 8, 8, 512)         2359808   
                                                                 
 block5_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   
                                                                 
 block5_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   
                                                                 
 block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         
                                                                 
=================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0
_________________________________________________________________
1/1 [==============================] - 0s 35ms/step
</pre></div>
</div>
<img alt="../_images/ModellingFinal_208_1.png" src="../_images/ModellingFinal_208_1.png" />
</div>
</div>
</section>
<section id="d-vgg16-evaluation">
<h3>d) VGG16 - Evaluation<a class="headerlink" href="#d-vgg16-evaluation" title="Permalink to this headline">#</a></h3>
<p>Nachfolgend wird das vortrainierte VGG16 Modell evaluiert. Hierfür beginnen wir mit der Betrachtung der Accuracy und des Losses über die Trainingsepochen.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy und Loss des vortrainierten VGG16 mit dataset d:&quot;</span><span class="p">)</span>
<span class="n">plot_accuracy</span><span class="p">(</span><span class="n">history6</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy und Loss des vortrainierten VGG16 mit dataset d:
Maximum accuracy:  0.7401103973388672
Minimum Loss:  0.839721143245697
</pre></div>
</div>
<img alt="../_images/ModellingFinal_211_1.png" src="../_images/ModellingFinal_211_1.png" />
</div>
</div>
<p>Wie bereits erwähnt, ist es auffällig, dass die Werte der Valdidierungsdaten für Accuracy und Loss besser sind als für die Trainingsdaten. Eine Erklärung hierfür könnte der Dropout Layer sein.<br />
In den ersten Epochen sind starke Verbesserungen in der Performance für beide Trainings- und Validationdaten zu erkennen. In den Validierungswerten sind danach einige Sprünge zu verzeichnen. Im Laufe der Epochen flachen alle Kurven immer mehr ab und pendeln sich nach ca. 8 Epochen langsam ein. Die Kurven lassen darauf schließen, dass weitere Epochen noch leichte Verbesserungen bewirken würden, eine starke Verbesserung ists aber nicht mehr anzunehmen. Daher wurde aus Laufzeitgründen auf weiteres Training verzichtet.</p>
<p>Im Anschluss werden die Testdaten mit dem VGG16 Modell klassifiziert.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">VGG16_corL</span><span class="p">,</span> <span class="n">VGG16_predL</span> <span class="o">=</span> <span class="n">predict_testdata</span><span class="p">(</span><span class="n">VGG16</span><span class="p">,</span> <span class="n">test_ds2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
1/1 [==============================] - 1s 1s/step
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">VGG16_corL</span><span class="p">,</span> <span class="n">VGG16_predL</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="n">class_names</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>              precision    recall  f1-score   support

     chicken       0.59      0.48      0.53       180
    cockatoo       0.50      0.77      0.60       161
       eagle       0.73      0.64      0.68       241
    flamingo       0.86      0.80      0.83       297
     ostrich       0.88      0.75      0.81       252
         owl       0.85      0.70      0.77       253
     penguin       0.68      0.80      0.73       208
         tit       0.74      0.85      0.79       286
       tucan       0.81      0.80      0.80       298

    accuracy                           0.74      2176
   macro avg       0.74      0.73      0.73      2176
weighted avg       0.76      0.74      0.74      2176
</pre></div>
</div>
</div>
</div>
<p>Der Classification Report zeigt die Werte der Testdaten aufgeteilt nach den verschiedenen Labeln.<br />
Auffällig ist, dass die ersten drei Klassen Chicken, Cockatoo und Eagle die schlechteste Performance (Precision, Recall und F1-Score) haben. Dies ist darin zu begründen, dass in dem verwendeten Datenset 2 von diesen 3 Klassen die wenigsten Bilder vorhanden sind (siehe Kapitel zu Dataset 2). Um die Werte in den 3 Klassen zu verbessern, sind somit zusätzliche Bilder von Chicken, Cockatoo und Eagle notwendig. Diese können entweder von weiteren Quellen heruntergerladen werden oder künstlich durch Image Augmentation erzeugt werden. Die im Kapitel zu Image Augmentation gezeigten Techniken müssten hierfür für diese Klassen angewendet und die Ergebnisse physisch gespeichert und dem Datenset hinzugefügt werden.<br />
Die Klasse Ostrich hat den besten Wert in Precision, die Klasse Tit für Recall und die Klasse Flamingo für F1-Score.<br />
Da für unseren Use Case weder Precicion noch Recall als wichtiger erachtet werden können, schneidet im arithmetischen Mittelwert der beiden Werte (F1-Score) die Klasse Flamingo am besten ab und wird somit als die beste vom Modell vorhergesagte Klasse deklariert.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">VGG16_corL</span><span class="p">,</span> <span class="n">VGG16_predL</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_217_0.png" src="../_images/ModellingFinal_217_0.png" />
</div>
</div>
<p>In der Confusion Matrix ist zu erkennen, dass grundsätzlich die meisten Bilder vom Modell korrekt klassifiziert wurden (diagonale Linie durch die Matrix zeigt hohe Werte). In den ersten 3 Klassen (Chicken, Cockatoo und Eagle) wurden im Vergleich zu den anderen Klassen mehr Bilder nicht korrekt klassifiziert, es gibt deutlich mehr FN und FP als in anderen Klassen. Auffällig ist hier beispielsweise, dass 42 Chicken Bilder als Cockatoo gelabeld wurden und 21 Eulen (Klasse 5) als Adler (Klasse 2). Wie bereits erwähnt, liegen für diese 3 Klassen die wenigsten Bilder vor. Um die Performance zu verbessern, bedarf es weitere Bilder dieser Klassen.<br />
Ansonsten sind die Werte breit gestreut und es ist nicht zu erkennen, dass beispielsweise zwei Klassen durch das Modell häufig miteinander verwechselt wurden.</p>
</section>
<section id="e-vgg16-fazit">
<h3>e) VGG16 - Fazit<a class="headerlink" href="#e-vgg16-fazit" title="Permalink to this headline">#</a></h3>
<p>Das vortrainierte VGG16 Modell mit eigenen fully-connected layern zeigt bisher das beste Ergebnis.<br />
Zudem wurden in der Evaluation noch weitere Optimierungspotenziale durch das Hinfügen weiterer Bilder der Klassen 1-3 sowie dem Trainieren zusätzlicher Epochen erkannt. Dadurch ist anzunehmen, dass sich die Performance in allen Metriken (Gesamt-Accuracy sowie Precision, Recall und F1-Score der einzelnen Klassen) noch weiter verbessern.<br />
Dieses Modell sollte somit in die finale Auswahl aufgenommen werden.</p>
</section>
</section>
<section id="model-5-pretrained-xception-neural-network">
<h2>6.5 Model 5 - Pretrained Xception Neural Network<a class="headerlink" href="#model-5-pretrained-xception-neural-network" title="Permalink to this headline">#</a></h2>
<section id="a-xception-introduction">
<h3>a) Xception - Introduction<a class="headerlink" href="#a-xception-introduction" title="Permalink to this headline">#</a></h3>
<p>Das Xception Modell ist eine Weiterentwicklung des Inception Modells. Wie das VGG16 Modell wurde auch dieses mit dem ImageNet Datenset trainiert und kann zwischen 1000 Klassen unterscheiden. Dieses Modell hat eine Tiefe von 71 Layern, ist also nochmal deutlich tiefer als das VGG16 Modell.<br />
Die Architektur ist nachfolgend dargestellt:</p>
<p><img alt="Architektur Xception Modell" src="_sources/Xception-architecture.PNG" /></p>
<p>(Bildquelle2: <a class="reference external" href="https://medium.com/analytics-vidhya/image-recognition-using-pre-trained-xception-model-in-5-steps-96ac858f4206">https://medium.com/analytics-vidhya/image-recognition-using-pre-trained-xception-model-in-5-steps-96ac858f4206</a>)</p>
<p>Auch bei diesem Pretrained Modell wird nur das Basismodell mit seinen Gewichten verwendet und an dessen Output eine eigene Architektur aus fully-connected layern angehängt.</p>
</section>
<section id="b-xception-construction">
<h3>b) Xception - Construction<a class="headerlink" href="#b-xception-construction" title="Permalink to this headline">#</a></h3>
<p>Wie bereits beim VGG116 Modell wird auch hier nur die Basis Architektur des Xception Modells mit seinen Gewichten geladen und an diese mit einer eigenen Architektur von fully-connected layern kombiniert.</p>
<p>Auch hier wurden verschiedene Architekturen von fully-connected layern, sowie Hyperparameter getestet. Es wird nachfolgend nur die Kombination gezeigt, welche hierbei das beste Ergebnis erzielte.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Xcept</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s1">&#39;Xcept-256px-128-256dense.h5&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Das Xcpetion Modell wurde ursprünglich mit einer Bildgröße von 299x299 Pixeln trainiert. Daher wird für dieses Modell eine größere Image_Size als in den vorherigen Modellen verwendet. Dafür muss zunächst das dataset 4 mit einer Image_Size von 256x256 Pixeln erstellt werden.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">image_size4</span><span class="o">=</span><span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span>
<span class="n">batch_size4</span><span class="o">=</span><span class="mi">128</span>
<span class="n">train_ds4</span><span class="p">,</span> <span class="n">val_ds4</span> <span class="o">=</span> <span class="n">create_datasets</span><span class="p">(</span><span class="n">image_size4</span><span class="p">,</span> <span class="n">batch_size4</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;C:\Users\olive\Documents\HdM\3. Semester\Machine Learning\Project Whale\Bilder2&quot;</span><span class="p">)</span>
<span class="n">test_ds4</span> <span class="o">=</span> <span class="n">val_ds4</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span>
<span class="n">val_ds4</span> <span class="o">=</span> <span class="n">val_ds4</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Found 14500 files belonging to 9 classes.
Using 10150 files for training.
Found 14500 files belonging to 9 classes.
Using 4350 files for validation.
</pre></div>
</div>
</div>
</div>
<p>Anschließend wird die Basisarchitektur des Xception Modells mit seinen Gewichten geladen und diese visualisiert.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">baseModelXception</span><span class="o">=</span> <span class="n">xception</span><span class="o">.</span><span class="n">Xception</span><span class="p">(</span><span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">include_top</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="s1">&#39;imagenet&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">visualkeras</span><span class="o">.</span><span class="n">layered_view</span><span class="p">(</span><span class="n">baseModelXception</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_231_0.png" src="../_images/ModellingFinal_231_0.png" />
</div>
</div>
<p>Es wird deutlich, dass das Xception Modell eine deutlich komplexere Architektur als alle zuvor erstellten Modelle besitzt. Es wechseln sich eine Vielzahl verschiedener Dense, Pooling, Activation und Batch Layer ab.<br />
Die Gewichte dieser Layer werden auch hier auf Trainable = False gestellt, sodass sie sich in der Trainingsphase nicht mehr verändern.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Gewichte in den Layern des Basis Modells einfrieren, sodass diese in der Trainingsphase nicht geupdatet werden</span>
<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">baseModelXception</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
    <span class="n">layer</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
<p>Zwar bietet auch dieses Modell eine eigene Preprocessing Methode, allerdings wurden auch hier schlechtere Ergebnisse erzielt, als wenn ein eigenes Preprocessing durchgeführt wird. Daher wird die Methode preprocess_input nicht verwendet.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">##for batch_images, batch_labels in train_ds:</span>
 <span class="c1">#   batch_images = xception.preprocess_input(batch_images)</span>
</pre></div>
</div>
</div>
</div>
<p>Da das Xception Modell ein Intervall der Bildpixel von [-1,1] erwartet, wird zunächst ein entsprechendes Rescaling durchgeführt und der Output an das Basis Modell übergeben. Anschließend folgen verschiedene fully-connected layers.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Xcept</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">Xcept</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Rescaling</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.</span><span class="o">/</span><span class="mf">127.5</span><span class="p">,</span> <span class="n">offset</span><span class="o">=-</span><span class="mf">1.</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span> <span class="p">,</span><span class="mi">3</span><span class="p">)))</span> <span class="c1">#Xception Model erwartet Pixelinter von [-1,1]</span>
<span class="n">Xcept</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">baseModelXception</span><span class="p">)</span>
<span class="n">Xcept</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPool2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="n">Xcept</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">Xcept</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>
<span class="n">Xcept</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
<span class="n">Xcept</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>
<span class="n">Xcept</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="n">Xcept</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">))</span>
<span class="n">Xcept</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_47&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 rescaling_3 (Rescaling)     (None, 256, 256, 3)       0         
                                                                 
 xception (Functional)       (None, 8, 8, 2048)        20861480  
                                                                 
 max_pooling2d_111 (MaxPooli  (None, 4, 4, 2048)       0         
 ng2D)                                                           
                                                                 
 flatten_39 (Flatten)        (None, 32768)             0         
                                                                 
 dense_99 (Dense)            (None, 128)               4194432   
                                                                 
 dropout_77 (Dropout)        (None, 128)               0         
                                                                 
 dense_100 (Dense)           (None, 256)               33024     
                                                                 
 dropout_78 (Dropout)        (None, 256)               0         
                                                                 
 dense_101 (Dense)           (None, 9)                 2313      
                                                                 
=================================================================
Total params: 25,091,249
Trainable params: 4,229,769
Non-trainable params: 20,861,480
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
</section>
<section id="c-xception-training">
<h3>c) Xception - Training<a class="headerlink" href="#c-xception-training" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">Xcept</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fitten des vortrainierten Xception Netz mit Dataset 4&quot;</span><span class="p">)</span>
<span class="n">history7</span> <span class="o">=</span> <span class="n">Xcept</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_ds4</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size4</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">val_ds4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fitten des vortrainierten Xception Netz mit Dataset 4
Epoch 1/10
80/80 [==============================] - 975s 12s/step - loss: 1.5072 - accuracy: 0.5030 - val_loss: 0.5257 - val_accuracy: 0.9026
Epoch 2/10
80/80 [==============================] - 1107s 14s/step - loss: 0.5780 - accuracy: 0.8343 - val_loss: 0.2672 - val_accuracy: 0.9322
Epoch 3/10
80/80 [==============================] - 1142s 14s/step - loss: 0.3735 - accuracy: 0.8949 - val_loss: 0.2163 - val_accuracy: 0.9348
Epoch 4/10
80/80 [==============================] - 1180s 15s/step - loss: 0.2944 - accuracy: 0.9164 - val_loss: 0.1940 - val_accuracy: 0.9452
Epoch 5/10
80/80 [==============================] - 1191s 15s/step - loss: 0.2486 - accuracy: 0.9260 - val_loss: 0.1683 - val_accuracy: 0.9487
Epoch 6/10
80/80 [==============================] - 1207s 15s/step - loss: 0.2145 - accuracy: 0.9395 - val_loss: 0.1720 - val_accuracy: 0.9522
Epoch 7/10
80/80 [==============================] - 1210s 15s/step - loss: 0.1927 - accuracy: 0.9453 - val_loss: 0.1509 - val_accuracy: 0.9522
Epoch 8/10
80/80 [==============================] - 1235s 15s/step - loss: 0.1775 - accuracy: 0.9476 - val_loss: 0.1634 - val_accuracy: 0.9496
Epoch 9/10
80/80 [==============================] - 1246s 16s/step - loss: 0.1583 - accuracy: 0.9556 - val_loss: 0.1683 - val_accuracy: 0.9443
Epoch 10/10
80/80 [==============================] - 1254s 16s/step - loss: 0.1470 - accuracy: 0.9585 - val_loss: 0.1519 - val_accuracy: 0.9591
</pre></div>
</div>
</div>
</div>
<p>Nach 10 Trainingsepochen liegen Trainings- und Validationaccuracy beide bei ca. 95%.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Xcept.save(&#39;Xcept-256px-128-256dense.h5&#39;)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># definiere modell als output nach ersten hidden layer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">baseModelXception</span><span class="o">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">baseModelXception</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">)</span>

<span class="c1"># Bild mit richtiger Image Size laden</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">load_img</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;..\ProjectBirdClassification\Bilder2\chicken\0K3D5SDTO0RJ.jpg&quot;</span><span class="p">,</span> <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>
<span class="c1"># Bild zu Array konvertieren</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">img_to_array</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="c1"># Dimension expandieren</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">expand_dims</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># Peprocess Bild für VGG16</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">xception</span><span class="o">.</span><span class="n">preprocess_input</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="c1"># erzeugen der Feature Maps für ausgewählten Layer</span>
<span class="n">feature_maps</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="c1"># alle Feature Maps plotten</span>
<span class="n">square</span><span class="o">=</span><span class="mi">8</span>
<span class="n">ix</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">):</span>
	<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
		<span class="n">ax</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">square</span><span class="p">,</span> <span class="n">square</span><span class="p">,</span> <span class="n">ix</span><span class="p">)</span>
		<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
		<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
		<span class="n">pyplot</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">ix</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
		<span class="n">ix</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1/1 [==============================] - 0s 43ms/step
</pre></div>
</div>
<img alt="../_images/ModellingFinal_243_1.png" src="../_images/ModellingFinal_243_1.png" />
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Figure size 1440x1440 with 0 Axes&gt;
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Figure size 1440x1440 with 0 Axes&gt;
</pre></div>
</div>
</div>
</div>
</section>
<section id="d-xception-evaluation">
<h3>d) Xception - Evaluation<a class="headerlink" href="#d-xception-evaluation" title="Permalink to this headline">#</a></h3>
<p>Für die Evaluation des Modells werden zunächst der Verlauf von Accuracy und Loss über die Epochen betrachtet.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy und Loss des vortrainierten Xception mit dataset 4 (alle Bilder und 256 Pixel):&quot;</span><span class="p">)</span>
<span class="n">plot_accuracy</span><span class="p">(</span><span class="n">history7</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy und Loss des vortrainierten Xception mit dataset 4 (alle Bilder und 256 Pixel):
Maximum accuracy:  0.9591304063796997
Minimum Loss:  0.15086688101291656
</pre></div>
</div>
<img alt="../_images/ModellingFinal_246_1.png" src="../_images/ModellingFinal_246_1.png" />
</div>
</div>
<p>In den Plots ist wieder eine starke Verbesserung von Accuracy und Loss in den ersten 3 Epochen zu erkennen. Mit der 4. Epoche werden die Verbesserungen kleiner.<br />
Auffällig ist, dass die Validation Accuracy und der Loss zunächst deutlich besser als die der Trainingsdaten sind. Dies könnte wie beim VGG16 Modell an den Dropout Layern liegen. In diesem Modell verringert sich dieser Unterschied im Laufe der Epochen deutlich und ist nach 8 Epochen kaum noch vorhanden. Nach 9 Epochen sind Accuracy und Loss der Trainingsdaten siginifikant geringer als die der Validierungsdaten.<br />
Es ist anzunehmen, dass das Trainieren weiterer Epochen keine deutliche Verbesserung mehr bringen würde.</p>
<p>Nun werden die Testdaten durch das Modell klasifiziert und anschließend die Classification Metrics und die Confusion Matrix für diese ausgegeben.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Xcept_corL</span><span class="p">,</span> <span class="n">Xcept_predL</span> <span class="o">=</span> <span class="n">predict_testdata</span><span class="p">(</span><span class="n">Xcept</span><span class="p">,</span> <span class="n">test_ds4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4/4 [==============================] - 13s 3s/step
4/4 [==============================] - 13s 3s/step
4/4 [==============================] - 13s 3s/step
4/4 [==============================] - 13s 3s/step
4/4 [==============================] - 13s 3s/step
4/4 [==============================] - 14s 3s/step
4/4 [==============================] - 14s 3s/step
4/4 [==============================] - 13s 3s/step
4/4 [==============================] - 13s 3s/step
4/4 [==============================] - 13s 3s/step
4/4 [==============================] - 13s 3s/step
4/4 [==============================] - 13s 3s/step
4/4 [==============================] - 13s 3s/step
4/4 [==============================] - 13s 3s/step
4/4 [==============================] - 13s 3s/step
4/4 [==============================] - 13s 3s/step
4/4 [==============================] - 13s 3s/step
4/4 [==============================] - 13s 3s/step
4/4 [==============================] - 13s 3s/step
4/4 [==============================] - 13s 3s/step
4/4 [==============================] - 13s 3s/step
4/4 [==============================] - 13s 3s/step
4/4 [==============================] - 13s 3s/step
4/4 [==============================] - 13s 3s/step
4/4 [==============================] - 13s 3s/step
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Xcept_corL</span><span class="p">,</span> <span class="n">Xcept_predL</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="n">class_names</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>              precision    recall  f1-score   support

     chicken       0.92      0.92      0.92       257
    cockatoo       0.95      0.93      0.94       238
       eagle       0.92      0.94      0.93       359
    flamingo       0.95      0.96      0.96       409
     ostrich       0.98      0.98      0.98       387
         owl       0.90      0.92      0.91       370
     penguin       0.96      0.97      0.96       288
         tit       0.98      0.97      0.97       459
       tucan       0.99      0.96      0.97       433

    accuracy                           0.95      3200
   macro avg       0.95      0.95      0.95      3200
weighted avg       0.95      0.95      0.95      3200
</pre></div>
</div>
</div>
</div>
<p>Für dieses Modell liegen alle Performance Metriken (Precision, Recall und F1-Score) über 90% und schneiden somit sehr gut ab.<br />
Die schlechtesten Werte verzeichnet die Klasse Owl. Auch für diese Klasse liegen vergleichsweise eher weniger Bilder vor. (siehe Kapitel zu dataset 2, hier wird zwar dataset 4 verwendet, dieses ist inhaltlich aber das dataset 2 nur mit anderen Pixelwerten). Dies kann aber nicht der einzige Grund sein, da für die Klassen 1-3 noch weniger Bilder vorhanden sind. Allerdings schneiden auch diese drei Klassen schlechter ab als die anderen. Klasse Penguin enthält ebenfalls weniger Bilder als Klasse Owl, schneidet aber trotzdem deutlich besser ab. Dass Owl trotzdem schlechter abschneidet, könnte in einer höheren Intraclass Differenz in Klasse Owl bzw. in einer geringeren Intraclass Differenz der Penguin begründet sein. Klasse Penguin weißt zudem sehr deutliche Merkmale auf (immer schwarz, weiße Vögel), weshalb diese Klasse womöglich auch mit verhältnismäßig wenig Bilder gut für das Modell erkennbar ist.
Die Gesamtperformance des Modells lässt sich daher vermutlich noch durch das Hinzufügen weiterer Bilder dieser Klassen verbessern.<br />
Den besten F1-Score hat die Klasse Ostrich, bei der Precision und Recall bei den gleichhohen Wert von 98% erzielen.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">Xcept_corL</span><span class="p">,</span> <span class="n">Xcept_predL</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ModellingFinal_252_0.png" src="../_images/ModellingFinal_252_0.png" />
</div>
</div>
<p>Passend zu den Classification Metrics zeigt auch die Confusion Matrix überwiegend korrekt vorhergesagte Werte.<br />
Die Klassen 1-3 sowie 5 beinhalten mehr FP und FN. Heraus sticht hierbei, dass die Klassen 2 (Eagle) und 5 (Owl) häufig verwechselt werden, da 15 Eulenbilder als Adler und 10 Adlerbilder als Eule klassifiziert wurden.</p>
<p>Wie bei Modell 2 lassen wir das Xcept Modell nun auch die separaten Testbilder klassifizieren:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predict_image</span><span class="p">(</span><span class="s2">&quot;..\ProjectBirdClassification\Testbilder\madagascar-penguin.jpg&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">],</span> <span class="n">Xcept</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1/1 [==============================] - 1s 628ms/step
{&#39;penguin&#39;: 0.99957556, &#39;tucan&#39;: 0.0002493362, &#39;owl&#39;: 6.0418548e-05, &#39;chicken&#39;: 3.477147e-05, &#39;tit&#39;: 2.8461069e-05, &#39;cockatoo&#39;: 2.2602657e-05, &#39;eagle&#39;: 2.054058e-05, &#39;flamingo&#39;: 6.7475426e-06, &#39;ostrich&#39;: 1.6372794e-06}
</pre></div>
</div>
<img alt="../_images/ModellingFinal_255_1.png" src="../_images/ModellingFinal_255_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predict_image</span><span class="p">(</span><span class="s2">&quot;..\ProjectBirdClassification\Testbilder\TucanHaribo.jpg&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">],</span> <span class="n">Xcept</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1/1 [==============================] - 0s 91ms/step
{&#39;tucan&#39;: 0.8486338, &#39;cockatoo&#39;: 0.051814474, &#39;tit&#39;: 0.027918724, &#39;chicken&#39;: 0.019426193, &#39;flamingo&#39;: 0.017281001, &#39;penguin&#39;: 0.016189197, &#39;owl&#39;: 0.007194806, &#39;ostrich&#39;: 0.006255712, &#39;eagle&#39;: 0.005286045}
</pre></div>
</div>
<img alt="../_images/ModellingFinal_256_1.png" src="../_images/ModellingFinal_256_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predict_image</span><span class="p">(</span><span class="s2">&quot;..\ProjectBirdClassification\Testbilder\Hedwig.jpg&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">],</span> <span class="n">Xcept</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1/1 [==============================] - 0s 90ms/step
{&#39;cockatoo&#39;: 0.5693151, &#39;owl&#39;: 0.37363514, &#39;eagle&#39;: 0.028552435, &#39;tucan&#39;: 0.006743378, &#39;chicken&#39;: 0.006610982, &#39;tit&#39;: 0.0045590443, &#39;flamingo&#39;: 0.0042679436, &#39;penguin&#39;: 0.0040656934, &#39;ostrich&#39;: 0.002250225}
</pre></div>
</div>
<img alt="../_images/ModellingFinal_257_1.png" src="../_images/ModellingFinal_257_1.png" />
</div>
</div>
<p>Wie Modell zwei können Pinguin und Tucan gut erkannt werden, allerdings tut sich das Xcept Modell genau wie Modell 2 mit der Eule schwer.</p>
</section>
<section id="e-xception-fazit">
<h3>e) Xception - Fazit<a class="headerlink" href="#e-xception-fazit" title="Permalink to this headline">#</a></h3>
<p>Das Xception Modell übertrifft das aktuelle VGG16 Modell nochmal deutlich in der Gesamt-Accuracy. Auch die Precicion, Recall und F1-Score Werte der einzelnen Klasse sind deutlich besser und unterscheiden sich zudem weniger stark zwischen den Klassen.</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="projekt-fazit">
<h1>7. Projekt Fazit<a class="headerlink" href="#projekt-fazit" title="Permalink to this headline">#</a></h1>
<p>Bei den selbst erstellten Modellen konnte das Ziel von 80% Accuracy nicht erreicht werden. Durch Tests und Optimierungen mit verschiedenen Architekturen und Parametern konnte eine Accuracy von knapp 70% erreicht werden. Durch das hinzufügen weiterer Trainingsdaten, vor allem für weniger stark vertretene Klassen, könnte dieser Wert vermutlich noch optimiert werden.</p>
<p>Bei den vortrainierten Modellen konnte wie erwartet ein deutlich besseres Ergebnis erzielt werden. Das Xception Modell lieferte mit einer Accuracy von ca. 95% das deutlich beste Ergebnis und wäre somit auch für den praktischen Einsatz geeignet. Auch die vortrainierten Netze könnten vermutlich mit mehr Ressourcen noch leicht optimiert werden.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="quellen">
<h1>8. Quellen<a class="headerlink" href="#quellen" title="Permalink to this headline">#</a></h1>
<p>zusätzliche Bilder: <a class="reference external" href="https://images.cv/">https://images.cv/</a><br />
Literatur zu Dropout: <a class="reference external" href="http://mipal.snu.ac.kr/images/1/16/Dropout_ACCV2016.pdf">http://mipal.snu.ac.kr/images/1/16/Dropout_ACCV2016.pdf</a><br />
Bildquelle 1: <a class="reference external" href="https://www.learndatasci.com/tutorials/hands-on-transfer-learning-keras/">https://www.learndatasci.com/tutorials/hands-on-transfer-learning-keras/</a><br />
Bildquelle 2: <a class="reference external" href="https://medium.com/analytics-vidhya/image-recognition-using-pre-trained-xception-model-in-5-steps-96ac858f4206">https://medium.com/analytics-vidhya/image-recognition-using-pre-trained-xception-model-in-5-steps-96ac858f4206</a></p>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./_sources"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Oliver Schabe & Annika Scheug<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>